{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sarp.utils import load_expert_data_hospital, separate_train_test, combine_nets, mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)],\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"2_sarp_repair.ipynb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of policy repair using SARP for the robot navigation in hospital. This script assumes that a pre-trained policy and a predictive model are already available. To pre-train a policy for this example run [0_pretrain_policy.py](0_pretrain_policy.py). Also to train a predictive model run [1_pretrain_predictive_model.py](1_pretrain_predictive_model.py). Here are the descriptions of models:\n",
    "- policy - input: the system state that includes the robot's goal, distancc and heading toward goal, and range sensor readings - output: linear and angular velocities.\n",
    "- predictive model - input: states and actions - output: collision [0, 1] or no collision [1, 0]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laod dataset\n",
    "First, we load the expert demonstrations for repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sample 1, goal: [10. 10.]\n",
      "loading sample 2, goal: [10. 10.]\n",
      "loading sample 3, goal: [10. 10.]\n",
      "loading sample 4, goal: [10. 10.]\n",
      "loading sample 5, goal: [10. 10.]\n",
      "loading sample 6, goal: [10. 10.]\n",
      "loading sample 7, goal: [10. 10.]\n",
      "loading sample 8, goal: [10. 10.]\n",
      "loading sample 9, goal: [10. 10.]\n",
      "loading sample 10, goal: [10. 10.]\n",
      "loading sample 11, goal: [-10.   5.]\n",
      "loading sample 12, goal: [-10.   5.]\n",
      "loading sample 13, goal: [-10.   5.]\n",
      "loading sample 14, goal: [-10.   5.]\n",
      "loading sample 15, goal: [-10.   5.]\n",
      "loading sample 16, goal: [-10.   5.]\n",
      "loading sample 17, goal: [-10.   5.]\n",
      "loading sample 18, goal: [-10.   5.]\n",
      "loading sample 19, goal: [-10.   5.]\n",
      "loading sample 20, goal: [-10.   5.]\n",
      "loading sample 21, goal: [10.  5.]\n",
      "loading sample 22, goal: [10.  5.]\n",
      "loading sample 23, goal: [10.  5.]\n",
      "loading sample 24, goal: [10.  5.]\n",
      "loading sample 25, goal: [10.  5.]\n",
      "loading sample 26, goal: [10.  5.]\n",
      "loading sample 27, goal: [10.  5.]\n",
      "loading sample 28, goal: [10.  5.]\n",
      "loading sample 29, goal: [10.  5.]\n",
      "loading sample 30, goal: [10.  5.]\n",
      "loading sample 31, goal: [-9. -9.]\n",
      "loading sample 32, goal: [-9. -9.]\n",
      "loading sample 33, goal: [-9. -9.]\n",
      "loading sample 34, goal: [-9. -9.]\n",
      "loading sample 35, goal: [-9. -9.]\n",
      "loading sample 36, goal: [-9. -9.]\n",
      "loading sample 37, goal: [-9. -9.]\n",
      "loading sample 38, goal: [-9. -9.]\n",
      "loading sample 39, goal: [-9. -9.]\n",
      "loading sample 40, goal: [-9. -9.]\n",
      "loading sample 41, goal: [ 9. -9.]\n",
      "loading sample 42, goal: [ 9. -9.]\n",
      "loading sample 43, goal: [ 9. -9.]\n",
      "loading sample 44, goal: [ 9. -9.]\n",
      "loading sample 45, goal: [ 9. -9.]\n",
      "loading sample 46, goal: [ 9. -9.]\n",
      "loading sample 47, goal: [ 9. -9.]\n",
      "loading sample 48, goal: [ 9. -9.]\n",
      "loading sample 49, goal: [ 9. -9.]\n",
      "loading sample 50, goal: [ 9. -9.]\n",
      "loading sample 51, goal: [-10.  10.]\n",
      "loading sample 52, goal: [-10.  10.]\n",
      "loading sample 53, goal: [-10.  10.]\n",
      "loading sample 54, goal: [-10.  10.]\n",
      "loading sample 55, goal: [-10.  10.]\n",
      "loading sample 56, goal: [-10.  10.]\n",
      "loading sample 57, goal: [-10.  10.]\n",
      "loading sample 58, goal: [-10.  10.]\n",
      "loading sample 59, goal: [-10.  10.]\n",
      "loading sample 60, goal: [-10.  10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 17:49:32.652077: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 17:49:33.237142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5120 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:d5:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# load the expert data\n",
    "data_dir = current_dir + f\"/data/expert_data\"\n",
    "num_samples = len(os.listdir(data_dir))\n",
    "\n",
    "state, action, _, property = load_expert_data_hospital(data_dir, num_samples, col_remove=True)\n",
    "state = [tf.convert_to_tensor(s, dtype=tf.float32) for s in state]\n",
    "action = [tf.convert_to_tensor(a, dtype=tf.float32) for a in action]\n",
    "property = [tf.convert_to_tensor(p, dtype=tf.float32) for p in property]\n",
    "train_data, test_data = separate_train_test([state, action, property], test_ratio=0.2)\n",
    "\n",
    "state_train, action_train, property_train = train_data\n",
    "state_test, action_test, property_test = test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and megre models\n",
    "Here, we load the policy and predictive models, then we merge them in a series fashion to be used in repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"repair_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " policy_layer_1 (Dense)      multiple                  3840      \n",
      "                                                                 \n",
      " policy_layer_2 (Dense)      multiple                  65792     \n",
      "                                                                 \n",
      " policy_layer_3 (Dense)      multiple                  514       \n",
      "                                                                 \n",
      " Predictive_layer_1 (Dense)  multiple                  4352      \n",
      "                                                                 \n",
      " Predictive_layer_2 (Dense)  multiple                  65792     \n",
      "                                                                 \n",
      " Predictive_layer_3 (Dense)  multiple                  2570      \n",
      "                                                                 \n",
      " Predictive_layer_4 (Dense)  multiple                  1408      \n",
      "                                                                 \n",
      " Predictive_layer_5 (Dense)  multiple                  16512     \n",
      "                                                                 \n",
      " Predictive_layer_6 (Dense)  multiple                  258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,038\n",
      "Trainable params: 161,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the models\n",
    "model_policy_orig = keras.models.load_model(\n",
    "    current_dir\n",
    "    + f\"/trained_models/policy/model\"\n",
    "    )\n",
    "model_predictive = keras.models.load_model(\n",
    "    current_dir\n",
    "    + f\"/trained_models/predictive_model/model\"\n",
    "    )\n",
    "\n",
    "# combine the models\n",
    "model_combined = combine_nets(model_policy_orig, model_predictive)\n",
    "\n",
    "# keep only the policy part of the combined model to be trained\n",
    "for layer in model_combined.layers:\n",
    "    if layer.name.split(\"_\")[0] == \"policy\":\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "_,_ = model_combined.predict(state[0][0:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimization parameters\n",
    "\n",
    "We first define the loss function, lagrangian penalty terms and the quadratic penalty terms. We assum two constraints:\n",
    "1. Constraint on linear velocity: $v\\leq0.9$         $\\Longrightarrow g_{vel} = ReLU(v-0.9)$\n",
    "2. Constraint on the output collision property: $\\psi = [1,0]$     $\\Longrightarrow g_{col} = \\psi[1]$\n",
    "\n",
    "The augmented loss is formulated as \n",
    "\n",
    "\\begin{align} \n",
    "\\mathcal{L}^a =  \\mathcal{L}_{original} -\\lambda_{col} g_{col} + \\frac{\\mu_{vel}}{2}g^2_{vel}-\\lambda_{col} g_{vel} + \\frac{\\mu_{vel}}{2}g^2_{vel}\\nonumber\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "params = {\n",
    "    \"lambda_collision\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_collision\": tf.constant(10.0, dtype=\"float32\"),\n",
    "    \"eta_collision\": tf.constant(0.001, dtype=\"float32\"),\n",
    "    \"beta_collision\": tf.constant(5, dtype=\"float32\"),\n",
    "    \"lambda_velocity\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "    \"eta_velocity\": tf.constant(0.001, dtype=\"float32\"),\n",
    "    \"beta_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "}\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create data batches\n",
    "batches = mini_batch(\n",
    "    tf.concat(state_train,0),\n",
    "    tf.concat(action_train,0), \n",
    "    tf.concat(property_train,0), \n",
    "    batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def col_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(y[:, 1]))\n",
    "\n",
    "def col_lagrangian(y):\n",
    "    return tf.reduce_sum(y[:, 1])\n",
    "\n",
    "def vel_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(tf.nn.relu(y[:, 0] - 0.9)))\n",
    "\n",
    "def vel_lagrangian(y):\n",
    "    return tf.reduce_sum(tf.nn.relu(y[:, 0] - 0.9))\n",
    "\n",
    "def augmented_loss(\n",
    "    s, a, params\n",
    "):\n",
    "    a_pred, p_pred = model_combined(s)\n",
    "    loss_value = (\n",
    "            100 * original_loss(a, a_pred)\n",
    "            - params[\"lambda_collision\"] * col_lagrangian(p_pred)\n",
    "            + params[\"mu_collision\"] / 2 * col_penalty(p_pred)\n",
    "            - params[\"lambda_velocity\"] * vel_lagrangian(a_pred)\n",
    "            + params[\"mu_velocity\"] / 2 * vel_penalty(a_pred)\n",
    "        )\n",
    "    return (\n",
    "            loss_value,\n",
    "            original_loss(a, a_pred),\n",
    "            col_lagrangian(p_pred),\n",
    "            vel_lagrangian(a_pred),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the optimizer and the policy update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler():\n",
    "    def __init__(self, optimizer, lr_min=5e-5, decay=0.1, patience=10, loss_tol=0.0001):\n",
    "        self.lr_min = lr_min\n",
    "        self.patience = patience\n",
    "        self.decay = decay\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tol = loss_tol\n",
    "        self.counter = 0\n",
    "        self.loss_prev = 10000\n",
    "\n",
    "    def on_batch_end(self, loss):\n",
    "        if self.loss_prev - loss > self.loss_tol:\n",
    "            pass\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                self.counter = 0\n",
    "                new_lr = self.optimizer.learning_rate * self.decay\n",
    "                if new_lr.numpy() >= self.lr_min:\n",
    "                    self.optimizer.learning_rate.assign(new_lr)\n",
    "        \n",
    "        self.loss_prev = loss\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "lr_scheduler = LearningRateScheduler(optimizer, lr_min=5e-5, decay=0.1, patience=10, loss_tol=0.0001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    s, a, params\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value, _, _, _ = augmented_loss(\n",
    "            s, a, params\n",
    "        )\n",
    "    grads = tape.gradient(loss_value, model_combined.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_combined.trainable_variables))\n",
    "    return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair the policy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we repair the policy in the loop and adjust the lagrangian multiplier and penalty coefficient accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCollector():\n",
    "    def __init__(self):\n",
    "        self.loss = {'train':[], 'test':[]}\n",
    "        self.col = {'train':[], 'test':[]}\n",
    "        self.vel = {'train':[], 'test':[]}\n",
    "        self.best_weights = []\n",
    "    \n",
    "    def update_state(self, s_train, a_train, s_test, a_test, params):\n",
    "        _, loss_train, col_train, vel_train = augmented_loss(\n",
    "            s_train, a_train, params\n",
    "        )\n",
    "        _, loss_test, col_test, vel_test = augmented_loss(\n",
    "            s_test, a_test, params\n",
    "        )\n",
    "        self.loss['train'].append(loss_train.numpy())\n",
    "        self.loss['test'].append(loss_test.numpy())\n",
    "        self.col['train'].append(col_train.numpy())\n",
    "        self.col['test'].append(col_test.numpy())\n",
    "        self.vel['train'].append(vel_train.numpy())\n",
    "        self.vel['test'].append(vel_test.numpy())\n",
    "    \n",
    "    def save_best_model(self, model):\n",
    "        if self.col['test'][-1] == min(self.col['test']):\n",
    "            self.best_weights = model.get_weights()[:(len(model.policy_arch)-1)*2]\n",
    "\n",
    "    def plot(self):\n",
    "        _, ax = plt.subplots(3,1, figsize=(10,10))\n",
    "        ax[0].plot(self.loss['train'], label='train')\n",
    "        ax[0].plot(self.loss['test'], label='test')\n",
    "        ax[0].set_ylabel('loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(self.col['train'], label='train')\n",
    "        ax[1].plot(self.col['test'], label='test')\n",
    "        ax[1].set_ylabel('collision')\n",
    "        ax[1].legend()\n",
    "        ax[2].plot(self.vel['train'], label='train')\n",
    "        ax[2].plot(self.vel['test'], label='test')\n",
    "        ax[2].set_ylabel('velocity')\n",
    "        ax[2].legend()\n",
    "        plt.show()\n",
    "\n",
    "class Verbose():\n",
    "    def __init__(self, metric_collector, optimizer, epochs):\n",
    "        self.metric_collector = metric_collector\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.best_model = None\n",
    "\n",
    "    def on_batch_end(self, epoch, model):\n",
    "        print(f\"e: {epoch}/{self.epochs}, lr: {self.optimizer.learning_rate.numpy():.6f}, loss: {self.metric_collector.loss['train'][-1]:.4f}, col: {self.metric_collector.col['train'][-1]:.4f}, vel: {self.metric_collector.vel['train'][-1]:.4f}, loss_val: {self.metric_collector.loss['test'][-1]:.4f}, col_val: {self.metric_collector.col['test'][-1]:.4f}, vel_val: {self.metric_collector.vel['test'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 1/100, lr: 0.001000, loss: 0.0244, col: 24.2153, vel: 383.5760, loss_val: 0.0254, col_val: 6.0159, vel_val: 102.3448\n",
      "e: 2/100, lr: 0.001000, loss: 0.0240, col: 23.9878, vel: 291.6478, loss_val: 0.0252, col_val: 6.8219, vel_val: 78.2289\n",
      "e: 3/100, lr: 0.001000, loss: 0.0236, col: 24.2473, vel: 244.4440, loss_val: 0.0259, col_val: 6.3245, vel_val: 65.6116\n",
      "e: 4/100, lr: 0.001000, loss: 0.0245, col: 23.8889, vel: 240.0183, loss_val: 0.0270, col_val: 6.1884, vel_val: 60.6659\n",
      "e: 5/100, lr: 0.001000, loss: 0.0224, col: 24.1060, vel: 256.0731, loss_val: 0.0255, col_val: 5.3001, vel_val: 66.8940\n",
      "e: 6/100, lr: 0.001000, loss: 0.0215, col: 21.9415, vel: 307.6382, loss_val: 0.0252, col_val: 5.4840, vel_val: 80.3945\n",
      "e: 7/100, lr: 0.001000, loss: 0.0227, col: 21.9743, vel: 290.7785, loss_val: 0.0264, col_val: 5.4926, vel_val: 78.0797\n",
      "e: 8/100, lr: 0.001000, loss: 0.0221, col: 22.2502, vel: 288.4465, loss_val: 0.0260, col_val: 5.2238, vel_val: 76.8418\n",
      "e: 9/100, lr: 0.001000, loss: 0.0226, col: 21.7395, vel: 324.1588, loss_val: 0.0271, col_val: 5.0343, vel_val: 85.5971\n",
      "e: 10/100, lr: 0.001000, loss: 0.0226, col: 22.6321, vel: 275.2657, loss_val: 0.0267, col_val: 5.0013, vel_val: 74.7178\n",
      "e: 11/100, lr: 0.001000, loss: 0.0291, col: 22.5443, vel: 99.9365, loss_val: 0.0327, col_val: 5.7668, vel_val: 26.0460\n",
      "e: 12/100, lr: 0.001000, loss: 0.0262, col: 19.5046, vel: 146.2131, loss_val: 0.0314, col_val: 4.5089, vel_val: 35.5446\n",
      "e: 13/100, lr: 0.001000, loss: 0.0223, col: 19.5163, vel: 204.6902, loss_val: 0.0288, col_val: 4.7851, vel_val: 52.6343\n",
      "e: 14/100, lr: 0.001000, loss: 0.0232, col: 18.7857, vel: 170.3269, loss_val: 0.0293, col_val: 5.3886, vel_val: 42.7552\n",
      "e: 15/100, lr: 0.001000, loss: 0.0239, col: 18.8204, vel: 141.0213, loss_val: 0.0296, col_val: 4.8537, vel_val: 34.9626\n",
      "e: 16/100, lr: 0.001000, loss: 0.0272, col: 16.9749, vel: 174.4992, loss_val: 0.0336, col_val: 4.8735, vel_val: 41.3014\n",
      "e: 17/100, lr: 0.001000, loss: 0.0234, col: 16.7089, vel: 214.1105, loss_val: 0.0306, col_val: 4.9192, vel_val: 54.7590\n",
      "e: 18/100, lr: 0.001000, loss: 0.0234, col: 17.0915, vel: 156.9056, loss_val: 0.0310, col_val: 4.9427, vel_val: 41.3655\n",
      "e: 19/100, lr: 0.001000, loss: 0.0238, col: 16.4953, vel: 148.7647, loss_val: 0.0306, col_val: 4.9059, vel_val: 38.8211\n",
      "e: 20/100, lr: 0.001000, loss: 0.0226, col: 16.3758, vel: 173.0866, loss_val: 0.0300, col_val: 4.3415, vel_val: 44.7733\n",
      "e: 21/100, lr: 0.001000, loss: 0.0437, col: 20.5024, vel: 70.0620, loss_val: 0.0492, col_val: 5.5023, vel_val: 17.6176\n",
      "e: 22/100, lr: 0.001000, loss: 0.0370, col: 17.2312, vel: 25.4987, loss_val: 0.0448, col_val: 6.1951, vel_val: 5.8288\n",
      "e: 23/100, lr: 0.000100, loss: 0.0315, col: 15.8375, vel: 33.5257, loss_val: 0.0401, col_val: 6.1898, vel_val: 7.6721\n",
      "e: 24/100, lr: 0.000100, loss: 0.0295, col: 15.4506, vel: 34.0241, loss_val: 0.0383, col_val: 6.0288, vel_val: 7.9640\n",
      "e: 25/100, lr: 0.000100, loss: 0.0282, col: 15.4354, vel: 35.2967, loss_val: 0.0369, col_val: 5.8854, vel_val: 8.5248\n",
      "e: 26/100, lr: 0.000100, loss: 0.0273, col: 15.3534, vel: 36.4628, loss_val: 0.0361, col_val: 5.7786, vel_val: 9.0815\n",
      "e: 27/100, lr: 0.000100, loss: 0.0264, col: 15.4748, vel: 39.0784, loss_val: 0.0350, col_val: 5.6290, vel_val: 10.0624\n",
      "e: 28/100, lr: 0.000100, loss: 0.0260, col: 15.2489, vel: 38.8521, loss_val: 0.0348, col_val: 5.5649, vel_val: 9.8379\n",
      "e: 29/100, lr: 0.000100, loss: 0.0254, col: 15.2932, vel: 40.3796, loss_val: 0.0342, col_val: 5.4437, vel_val: 10.4725\n",
      "e: 30/100, lr: 0.000100, loss: 0.0251, col: 15.1878, vel: 39.9183, loss_val: 0.0342, col_val: 5.4385, vel_val: 10.1684\n",
      "e: 31/100, lr: 0.000100, loss: 0.0266, col: 14.8378, vel: 13.2812, loss_val: 0.0352, col_val: 5.8681, vel_val: 3.0984\n",
      "e: 32/100, lr: 0.000100, loss: 0.0261, col: 15.0250, vel: 12.3141, loss_val: 0.0353, col_val: 5.7718, vel_val: 2.9072\n",
      "e: 33/100, lr: 0.000100, loss: 0.0265, col: 14.5057, vel: 12.0377, loss_val: 0.0356, col_val: 5.7670, vel_val: 2.8988\n",
      "e: 34/100, lr: 0.000100, loss: 0.0257, col: 14.6280, vel: 12.2191, loss_val: 0.0346, col_val: 5.6046, vel_val: 3.0563\n",
      "e: 35/100, lr: 0.000100, loss: 0.0255, col: 14.5628, vel: 12.0144, loss_val: 0.0349, col_val: 5.5786, vel_val: 3.0226\n",
      "e: 36/100, lr: 0.000100, loss: 0.0252, col: 14.5185, vel: 12.3679, loss_val: 0.0344, col_val: 5.3968, vel_val: 3.2538\n",
      "e: 37/100, lr: 0.000100, loss: 0.0250, col: 14.5788, vel: 13.0159, loss_val: 0.0340, col_val: 5.3005, vel_val: 3.5044\n",
      "e: 38/100, lr: 0.000100, loss: 0.0248, col: 14.5187, vel: 12.8390, loss_val: 0.0341, col_val: 5.3782, vel_val: 3.5585\n",
      "e: 39/100, lr: 0.000100, loss: 0.0250, col: 14.3420, vel: 13.4436, loss_val: 0.0342, col_val: 5.4066, vel_val: 3.7787\n",
      "e: 40/100, lr: 0.000100, loss: 0.0251, col: 14.2924, vel: 15.6291, loss_val: 0.0348, col_val: 5.6184, vel_val: 4.5652\n",
      "e: 41/100, lr: 0.000100, loss: 0.0262, col: 14.2209, vel: 8.7927, loss_val: 0.0353, col_val: 5.5735, vel_val: 2.6160\n",
      "e: 42/100, lr: 0.000100, loss: 0.0264, col: 13.4650, vel: 11.8292, loss_val: 0.0365, col_val: 5.8083, vel_val: 3.4832\n",
      "e: 43/100, lr: 0.000100, loss: 0.0272, col: 12.8211, vel: 11.4835, loss_val: 0.0378, col_val: 6.1429, vel_val: 3.6214\n",
      "e: 44/100, lr: 0.000100, loss: 0.0274, col: 12.5101, vel: 9.3361, loss_val: 0.0380, col_val: 5.8972, vel_val: 3.0056\n",
      "e: 45/100, lr: 0.000100, loss: 0.0271, col: 12.4543, vel: 8.4250, loss_val: 0.0377, col_val: 6.0976, vel_val: 2.7960\n",
      "e: 46/100, lr: 0.000100, loss: 0.0272, col: 12.5315, vel: 7.3177, loss_val: 0.0374, col_val: 6.1502, vel_val: 2.3283\n",
      "e: 47/100, lr: 0.000100, loss: 0.0270, col: 12.3733, vel: 8.4441, loss_val: 0.0376, col_val: 6.1935, vel_val: 2.7889\n",
      "e: 48/100, lr: 0.000100, loss: 0.0269, col: 12.4931, vel: 7.1438, loss_val: 0.0374, col_val: 6.1737, vel_val: 2.3773\n",
      "e: 49/100, lr: 0.000100, loss: 0.0269, col: 12.4408, vel: 7.2943, loss_val: 0.0374, col_val: 6.0384, vel_val: 2.5224\n",
      "e: 50/100, lr: 0.000100, loss: 0.0269, col: 12.3575, vel: 8.2733, loss_val: 0.0377, col_val: 6.1565, vel_val: 2.9323\n",
      "e: 51/100, lr: 0.000100, loss: 0.0283, col: 12.5775, vel: 5.9822, loss_val: 0.0385, col_val: 6.5374, vel_val: 2.1473\n",
      "e: 52/100, lr: 0.000100, loss: 0.0283, col: 12.1719, vel: 7.0699, loss_val: 0.0390, col_val: 6.3888, vel_val: 2.6509\n",
      "e: 53/100, lr: 0.000100, loss: 0.0281, col: 12.3719, vel: 5.0797, loss_val: 0.0381, col_val: 6.3901, vel_val: 1.7820\n",
      "e: 54/100, lr: 0.000100, loss: 0.0278, col: 12.2544, vel: 5.2851, loss_val: 0.0383, col_val: 6.2734, vel_val: 2.1618\n",
      "e: 55/100, lr: 0.000100, loss: 0.0278, col: 12.2708, vel: 5.0756, loss_val: 0.0382, col_val: 6.3037, vel_val: 2.0203\n",
      "e: 56/100, lr: 0.000100, loss: 0.0275, col: 12.3961, vel: 4.6271, loss_val: 0.0376, col_val: 6.1614, vel_val: 2.0024\n",
      "e: 57/100, lr: 0.000100, loss: 0.0274, col: 12.1940, vel: 5.0677, loss_val: 0.0377, col_val: 6.2929, vel_val: 2.1504\n",
      "e: 58/100, lr: 0.000100, loss: 0.0273, col: 12.2688, vel: 4.8554, loss_val: 0.0376, col_val: 6.3877, vel_val: 2.1933\n",
      "e: 59/100, lr: 0.000100, loss: 0.0276, col: 12.1673, vel: 5.7439, loss_val: 0.0377, col_val: 6.3673, vel_val: 2.4801\n",
      "e: 60/100, lr: 0.000100, loss: 0.0274, col: 12.3589, vel: 4.6722, loss_val: 0.0373, col_val: 6.3062, vel_val: 1.9584\n",
      "e: 61/100, lr: 0.000100, loss: 0.0300, col: 12.1743, vel: 6.4494, loss_val: 0.0400, col_val: 7.3310, vel_val: 2.0191\n",
      "e: 62/100, lr: 0.000100, loss: 0.0296, col: 12.2165, vel: 4.8591, loss_val: 0.0394, col_val: 6.9166, vel_val: 1.9701\n",
      "e: 63/100, lr: 0.000100, loss: 0.0292, col: 12.1239, vel: 4.7821, loss_val: 0.0390, col_val: 7.1973, vel_val: 1.8180\n",
      "e: 64/100, lr: 0.000100, loss: 0.0290, col: 12.2268, vel: 4.4491, loss_val: 0.0384, col_val: 6.9293, vel_val: 1.7939\n",
      "e: 65/100, lr: 0.000100, loss: 0.0288, col: 12.1363, vel: 4.7725, loss_val: 0.0384, col_val: 6.9469, vel_val: 1.8642\n",
      "e: 66/100, lr: 0.000100, loss: 0.0289, col: 12.3116, vel: 4.2616, loss_val: 0.0379, col_val: 6.7449, vel_val: 1.5456\n",
      "e: 67/100, lr: 0.000100, loss: 0.0286, col: 12.0454, vel: 4.8818, loss_val: 0.0381, col_val: 6.9128, vel_val: 1.7099\n",
      "e: 68/100, lr: 0.000100, loss: 0.0287, col: 12.0112, vel: 5.4697, loss_val: 0.0388, col_val: 6.9600, vel_val: 2.1718\n",
      "e: 69/100, lr: 0.000100, loss: 0.0290, col: 12.2504, vel: 4.2407, loss_val: 0.0382, col_val: 6.6865, vel_val: 1.5065\n",
      "e: 70/100, lr: 0.000100, loss: 0.0287, col: 12.0091, vel: 5.0323, loss_val: 0.0384, col_val: 6.8144, vel_val: 2.0047\n",
      "e: 71/100, lr: 0.000100, loss: 0.0300, col: 12.5060, vel: 4.3363, loss_val: 0.0396, col_val: 6.6232, vel_val: 1.6606\n",
      "e: 72/100, lr: 0.000100, loss: 0.0302, col: 11.7834, vel: 6.7749, loss_val: 0.0404, col_val: 6.1650, vel_val: 2.4213\n",
      "e: 73/100, lr: 0.000100, loss: 0.0313, col: 12.1596, vel: 4.4413, loss_val: 0.0411, col_val: 6.6345, vel_val: 1.5236\n",
      "e: 74/100, lr: 0.000100, loss: 0.0309, col: 12.0053, vel: 4.5793, loss_val: 0.0404, col_val: 6.2838, vel_val: 1.5518\n",
      "e: 75/100, lr: 0.000100, loss: 0.0310, col: 12.1107, vel: 4.4418, loss_val: 0.0401, col_val: 5.7622, vel_val: 1.5086\n",
      "e: 76/100, lr: 0.000100, loss: 0.0306, col: 11.9616, vel: 4.9237, loss_val: 0.0401, col_val: 5.6712, vel_val: 1.5494\n",
      "e: 77/100, lr: 0.000100, loss: 0.0313, col: 12.1103, vel: 4.1550, loss_val: 0.0406, col_val: 5.6926, vel_val: 1.3305\n",
      "e: 78/100, lr: 0.000100, loss: 0.0310, col: 12.1024, vel: 4.4322, loss_val: 0.0405, col_val: 5.3630, vel_val: 1.4749\n",
      "e: 79/100, lr: 0.000100, loss: 0.0315, col: 11.7989, vel: 5.7959, loss_val: 0.0413, col_val: 5.5045, vel_val: 1.9846\n",
      "e: 80/100, lr: 0.000100, loss: 0.0316, col: 12.1233, vel: 4.0319, loss_val: 0.0409, col_val: 5.6030, vel_val: 1.3603\n",
      "e: 81/100, lr: 0.000100, loss: 0.0333, col: 12.0266, vel: 5.1652, loss_val: 0.0431, col_val: 5.5659, vel_val: 1.6578\n",
      "e: 82/100, lr: 0.000100, loss: 0.0329, col: 11.8622, vel: 5.4178, loss_val: 0.0433, col_val: 5.6160, vel_val: 1.7368\n",
      "e: 83/100, lr: 0.000100, loss: 0.0334, col: 12.0868, vel: 4.2427, loss_val: 0.0435, col_val: 5.6476, vel_val: 1.4052\n",
      "e: 84/100, lr: 0.000100, loss: 0.0337, col: 12.0060, vel: 4.6474, loss_val: 0.0440, col_val: 5.2565, vel_val: 1.3764\n",
      "e: 85/100, lr: 0.000100, loss: 0.0341, col: 11.9885, vel: 4.3847, loss_val: 0.0441, col_val: 5.4003, vel_val: 1.2951\n",
      "e: 86/100, lr: 0.000100, loss: 0.0344, col: 12.0211, vel: 4.2530, loss_val: 0.0444, col_val: 5.0282, vel_val: 1.2715\n",
      "e: 87/100, lr: 0.000100, loss: 0.0339, col: 11.7899, vel: 5.1643, loss_val: 0.0441, col_val: 5.1920, vel_val: 1.6885\n",
      "e: 88/100, lr: 0.000100, loss: 0.0349, col: 12.0261, vel: 4.0219, loss_val: 0.0446, col_val: 5.5329, vel_val: 1.3347\n",
      "e: 89/100, lr: 0.000100, loss: 0.0352, col: 11.9197, vel: 4.4136, loss_val: 0.0450, col_val: 5.4749, vel_val: 1.3221\n",
      "e: 90/100, lr: 0.000100, loss: 0.0352, col: 11.9550, vel: 4.2586, loss_val: 0.0453, col_val: 5.1246, vel_val: 1.3724\n",
      "e: 91/100, lr: 0.000100, loss: 0.0370, col: 11.9790, vel: 5.2629, loss_val: 0.0478, col_val: 5.6274, vel_val: 1.5762\n",
      "e: 92/100, lr: 0.000100, loss: 0.0381, col: 11.9116, vel: 5.3650, loss_val: 0.0499, col_val: 5.7395, vel_val: 1.8147\n",
      "e: 93/100, lr: 0.000100, loss: 0.0384, col: 11.9552, vel: 4.3985, loss_val: 0.0501, col_val: 5.8697, vel_val: 1.4820\n",
      "e: 94/100, lr: 0.000100, loss: 0.0387, col: 11.8690, vel: 4.2031, loss_val: 0.0501, col_val: 5.7810, vel_val: 1.3936\n",
      "e: 95/100, lr: 0.000100, loss: 0.0385, col: 11.9171, vel: 4.1523, loss_val: 0.0499, col_val: 5.3278, vel_val: 1.2839\n",
      "e: 96/100, lr: 0.000100, loss: 0.0397, col: 11.8279, vel: 4.1016, loss_val: 0.0512, col_val: 5.6501, vel_val: 1.2910\n",
      "e: 97/100, lr: 0.000100, loss: 0.0399, col: 11.8475, vel: 4.0618, loss_val: 0.0515, col_val: 5.8833, vel_val: 1.2831\n",
      "e: 98/100, lr: 0.000100, loss: 0.0401, col: 11.8004, vel: 4.1795, loss_val: 0.0519, col_val: 5.3521, vel_val: 1.3156\n",
      "e: 99/100, lr: 0.000100, loss: 0.0398, col: 11.6970, vel: 4.5017, loss_val: 0.0519, col_val: 5.5350, vel_val: 1.6105\n",
      "e: 100/100, lr: 0.000100, loss: 0.0407, col: 11.8899, vel: 4.1256, loss_val: 0.0526, col_val: 5.2866, vel_val: 1.2261\n"
     ]
    }
   ],
   "source": [
    "metric_collector = MetricCollector()\n",
    "verbose = Verbose(metric_collector, optimizer, epochs)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in batches:\n",
    "        batch_loss = train_step(batch[0], batch[1], params)\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "    # update stats\n",
    "    metric_collector.update_state(\n",
    "        tf.concat(state_train,0), \n",
    "        tf.concat(action_train,0), \n",
    "        tf.concat(state_test,0), \n",
    "        tf.concat(action_test,0), \n",
    "        params,\n",
    "    ) \n",
    "\n",
    "    # save best model\n",
    "    metric_collector.save_best_model(model_combined)\n",
    "\n",
    "    # update parameters\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        params[\"lambda_collision\"] = (\n",
    "            params[\"lambda_collision\"]\n",
    "            + params[\"eta_collision\"] * metric_collector.col[\"train\"][-1]\n",
    "        )\n",
    "        params[\"mu_collision\"] = params[\"mu_collision\"] * params[\"beta_collision\"]\n",
    "        params[\"lambda_velocity\"] = (\n",
    "            params[\"lambda_velocity\"]\n",
    "            + params[\"eta_velocity\"] * metric_collector.vel[\"train\"][-1]\n",
    "        )\n",
    "        params[\"mu_velocity\"] = params[\"mu_velocity\"] * params[\"beta_velocity\"]\n",
    "    \n",
    "    # print stats\n",
    "    verbose.on_batch_end(epoch+1, model_combined)\n",
    "    \n",
    "    # update learning rate\n",
    "    lr_scheduler.on_batch_end(metric_collector.col['test'][-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /home/local/ASUAD/kmajd1/SARP/examples/1_hospital_simulation/trained_models/repaired_policy/model/assets\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(current_dir + f\"/trained_models/repaired_policy\"):\n",
    "    os.makedirs(current_dir + f\"/trained_models/repaired_policy\")\n",
    "\n",
    "counter = 0\n",
    "for l in range(len(model_policy_orig.layers)):\n",
    "    if (len(model_policy_orig.layers[l].get_weights())) > 0:\n",
    "        model_policy_orig.layers[l].set_weights(\n",
    "            [metric_collector.best_weights[2*counter], metric_collector.best_weights[2*counter+1]]\n",
    "        )\n",
    "        counter += 1\n",
    "\n",
    "keras.models.save_model(\n",
    "    model_policy_orig,\n",
    "    f\"{current_dir}/trained_models/repaired_policy/model\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=False,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None,\n",
    "    save_traces=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
