{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sarp.utils import load_expert_data_hospital, separate_train_test, combine_nets, mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)],\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "currn_dir = os.path.dirname(os.path.abspath(\"2_sarp_repair.ipynb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of policy repair using SARP for the robot navigation in hospital. This script assumes that a pre-trained policy and a predictive model are already available. To pre-train a policy for this example run [0_pretrain_policy.py](0_pretrain_policy.py). Also to train a predictive model run [1_pretrain_predictive_model.py](1_pretrain_predictive_model.py). Here are the descriptions of models:\n",
    "- policy - input: the system state that includes the robot's goal, distancc and heading toward goal, and range sensor readings - output: linear and angular velocities.\n",
    "- predictive model - input: states and actions - output: collision [0, 1] or no collision [1, 0]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laod dataset\n",
    "First, we load the expert demonstrations for repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sample 1, goal: [10. 10.]\n",
      "loading sample 2, goal: [10. 10.]\n",
      "loading sample 3, goal: [10. 10.]\n",
      "loading sample 4, goal: [10. 10.]\n",
      "loading sample 5, goal: [10. 10.]\n",
      "loading sample 6, goal: [10. 10.]\n",
      "loading sample 7, goal: [10. 10.]\n",
      "loading sample 8, goal: [10. 10.]\n",
      "loading sample 9, goal: [10. 10.]\n",
      "loading sample 10, goal: [10. 10.]\n",
      "loading sample 11, goal: [-10.   5.]\n",
      "loading sample 12, goal: [-10.   5.]\n",
      "loading sample 13, goal: [-10.   5.]\n",
      "loading sample 14, goal: [-10.   5.]\n",
      "loading sample 15, goal: [-10.   5.]\n",
      "loading sample 16, goal: [-10.   5.]\n",
      "loading sample 17, goal: [-10.   5.]\n",
      "loading sample 18, goal: [-10.   5.]\n",
      "loading sample 19, goal: [-10.   5.]\n",
      "loading sample 20, goal: [-10.   5.]\n",
      "loading sample 21, goal: [10.  5.]\n",
      "loading sample 22, goal: [10.  5.]\n",
      "loading sample 23, goal: [10.  5.]\n",
      "loading sample 24, goal: [10.  5.]\n",
      "loading sample 25, goal: [10.  5.]\n",
      "loading sample 26, goal: [10.  5.]\n",
      "loading sample 27, goal: [10.  5.]\n",
      "loading sample 28, goal: [10.  5.]\n",
      "loading sample 29, goal: [10.  5.]\n",
      "loading sample 30, goal: [10.  5.]\n",
      "loading sample 31, goal: [-9. -9.]\n",
      "loading sample 32, goal: [-9. -9.]\n",
      "loading sample 33, goal: [-9. -9.]\n",
      "loading sample 34, goal: [-9. -9.]\n",
      "loading sample 35, goal: [-9. -9.]\n",
      "loading sample 36, goal: [-9. -9.]\n",
      "loading sample 37, goal: [-9. -9.]\n",
      "loading sample 38, goal: [-9. -9.]\n",
      "loading sample 39, goal: [-9. -9.]\n",
      "loading sample 40, goal: [-9. -9.]\n",
      "loading sample 41, goal: [ 9. -9.]\n",
      "loading sample 42, goal: [ 9. -9.]\n",
      "loading sample 43, goal: [ 9. -9.]\n",
      "loading sample 44, goal: [ 9. -9.]\n",
      "loading sample 45, goal: [ 9. -9.]\n",
      "loading sample 46, goal: [ 9. -9.]\n",
      "loading sample 47, goal: [ 9. -9.]\n",
      "loading sample 48, goal: [ 9. -9.]\n",
      "loading sample 49, goal: [ 9. -9.]\n",
      "loading sample 50, goal: [ 9. -9.]\n",
      "loading sample 51, goal: [-10.  10.]\n",
      "loading sample 52, goal: [-10.  10.]\n",
      "loading sample 53, goal: [-10.  10.]\n",
      "loading sample 54, goal: [-10.  10.]\n",
      "loading sample 55, goal: [-10.  10.]\n",
      "loading sample 56, goal: [-10.  10.]\n",
      "loading sample 57, goal: [-10.  10.]\n",
      "loading sample 58, goal: [-10.  10.]\n",
      "loading sample 59, goal: [-10.  10.]\n",
      "loading sample 60, goal: [-10.  10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 21:16:59.290433: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# load the expert data\n",
    "data_dir = currn_dir + f\"/data/expert_data\"\n",
    "num_samples = len(os.listdir(data_dir))\n",
    "\n",
    "state, action, _, property = load_expert_data_hospital(data_dir, num_samples)\n",
    "state = [tf.convert_to_tensor(s, dtype=tf.float32) for s in state]\n",
    "action = [tf.convert_to_tensor(a, dtype=tf.float32) for a in action]\n",
    "property = [tf.convert_to_tensor(p, dtype=tf.float32) for p in property]\n",
    "train_data, test_data = separate_train_test([state, action, property], test_ratio=0.2)\n",
    "\n",
    "state_train, action_train, property_train = train_data\n",
    "state_test, action_test, property_test = test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and megre models\n",
    "Here, we load the policy and predictive models, then we merge them in a series fashion to be used in repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"repair_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " policy_layer_1 (Dense)      multiple                  3840      \n",
      "                                                                 \n",
      " policy_layer_2 (Dense)      multiple                  65792     \n",
      "                                                                 \n",
      " policy_layer_3 (Dense)      multiple                  514       \n",
      "                                                                 \n",
      " Predictive_layer_1 (Dense)  multiple                  4352      \n",
      "                                                                 \n",
      " Predictive_layer_2 (Dense)  multiple                  65792     \n",
      "                                                                 \n",
      " Predictive_layer_3 (Dense)  multiple                  2570      \n",
      "                                                                 \n",
      " Predictive_layer_4 (Dense)  multiple                  1408      \n",
      "                                                                 \n",
      " Predictive_layer_5 (Dense)  multiple                  16512     \n",
      "                                                                 \n",
      " Predictive_layer_6 (Dense)  multiple                  258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,038\n",
      "Trainable params: 161,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the models\n",
    "model_policy_orig = keras.models.load_model(\n",
    "    currn_dir\n",
    "    + f\"/trained_models/policy/model\"\n",
    "    )\n",
    "model_predictive = keras.models.load_model(\n",
    "    currn_dir\n",
    "    + f\"/trained_models/predictive_model/model\"\n",
    "    )\n",
    "\n",
    "# combine the models\n",
    "model_combined = combine_nets(model_policy_orig, model_predictive)\n",
    "\n",
    "# keep only the policy part of the combined model to be trained\n",
    "for layer in model_combined.layers:\n",
    "    if layer.name.split(\"_\")[0] == \"policy\":\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "_,_ = model_combined.predict(state[0][0:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimization parameters\n",
    "\n",
    "We first define the loss function, lagrangian penalty terms and the quadratic penalty terms. We assum two constraints:\n",
    "1. Constraint on linear velocity: $v\\leq0.9$\n",
    "2. Constraint on the output collision property: $\\psi = [1,0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "learning_rate = 0.00005\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "params = {\n",
    "    \"lambda_collision\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_collision\": tf.constant(5.0, dtype=\"float32\"),\n",
    "    \"eta_collision\": tf.constant(0.01, dtype=\"float32\"),\n",
    "    \"beta_collision\": tf.constant(10, dtype=\"float32\"),\n",
    "    \"lambda_velocity\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_velocity\": tf.constant(10.0, dtype=\"float32\"),\n",
    "    \"eta_velocity\": tf.constant(0.0001, dtype=\"float32\"),\n",
    "    \"beta_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "}\n",
    "\n",
    "# create data batches\n",
    "batches = mini_batch(\n",
    "    tf.concat(state_train,0),\n",
    "    tf.concat(action_train,0), \n",
    "    tf.concat(property_train,0), \n",
    "    batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def col_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(y[:, 1]))\n",
    "\n",
    "def col_lagrangian(y):\n",
    "    return tf.reduce_sum(y[:, 1])\n",
    "\n",
    "def vel_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(tf.nn.relu(y[:, 0] - 0.9)))\n",
    "\n",
    "def vel_lagrangian(y):\n",
    "    return tf.reduce_sum(tf.nn.relu(y[:, 0] - 0.9))\n",
    "\n",
    "def augmented_loss(\n",
    "    s, a, params\n",
    "):\n",
    "    a_pred, p_pred = model_combined(s)\n",
    "    loss_value = (\n",
    "            100 * original_loss(a, a_pred)\n",
    "            - params[\"lambda_collision\"] * col_lagrangian(p_pred)\n",
    "            + params[\"mu_collision\"] / 2 * col_penalty(p_pred)\n",
    "            - params[\"lambda_velocity\"] * vel_lagrangian(a_pred)\n",
    "            + params[\"mu_velocity\"] / 2 * vel_penalty(a_pred)\n",
    "        )\n",
    "    return (\n",
    "            loss_value,\n",
    "            original_loss(a, a_pred),\n",
    "            col_lagrangian(p_pred),\n",
    "            vel_lagrangian(a_pred),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the optimizer and the policy update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler():\n",
    "    def __init__(self, lr, decay, patience):\n",
    "        self.lr = lr\n",
    "        self.patience = patience\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.iterations += 1\n",
    "        self.model.optimizer.lr = self.lr * self.decay ** self.iterations\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    s, a, params\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value, _, _, _ = augmented_loss(\n",
    "            s, a, params\n",
    "        )\n",
    "    grads = tape.gradient(loss_value, model_combined.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_combined.trainable_variables))\n",
    "    return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair the policy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we repair the policy in the loop and adjust the lagrangian multiplier and penalty coefficient accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCollector():\n",
    "    def __init__(self):\n",
    "        self.loss = {'train':[], 'test':[]}\n",
    "        self.col = {'train':[], 'test':[]}\n",
    "        self.vel = {'train':[], 'test':[]}\n",
    "    \n",
    "    def update_state(self, s_train, a_train, s_test, a_test, params):\n",
    "        _, loss_train, col_train, vel_train = augmented_loss(\n",
    "            s_train, a_train, params\n",
    "        )\n",
    "        _, loss_test, col_test, vel_test = augmented_loss(\n",
    "            s_test, a_test, params\n",
    "        )\n",
    "        self.loss['train'].append(loss_train.numpy())\n",
    "        self.loss['test'].append(loss_test.numpy())\n",
    "        self.col['train'].append(col_train.numpy())\n",
    "        self.col['test'].append(col_test.numpy())\n",
    "        self.vel['train'].append(vel_train.numpy())\n",
    "        self.vel['test'].append(vel_test.numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.802249834065982\n",
      "Epoch 2 loss: 2.5747073712787256\n",
      "Epoch 3 loss: 2.5286790428055363\n",
      "Epoch 4 loss: 2.496774997551793\n",
      "Epoch 5 loss: 2.470918023154596\n",
      "Epoch 6 loss: 2.448998411385794\n",
      "Epoch 7 loss: 2.4297337439066853\n",
      "Epoch 8 loss: 2.4129846089397633\n",
      "Epoch 9 loss: 2.398401116926358\n",
      "Epoch 10 loss: 2.3850675705083564\n",
      "Epoch 11 loss: 6.083901407990947\n",
      "Epoch 12 loss: 5.956256664563022\n",
      "Epoch 13 loss: 5.898814251827995\n",
      "Epoch 14 loss: 5.844591911124652\n",
      "Epoch 15 loss: 5.810770613901463\n",
      "Epoch 16 loss: 5.784766577080432\n",
      "Epoch 17 loss: 5.7625008160689415\n",
      "Epoch 18 loss: 5.742411918958914\n",
      "Epoch 19 loss: 5.722550161037605\n",
      "Epoch 20 loss: 5.7005829452472145\n",
      "Epoch 21 loss: 39.26186564240947\n",
      "Epoch 22 loss: 38.441670112291085\n",
      "Epoch 23 loss: 37.802410667653206\n",
      "Epoch 24 loss: 37.31985822162256\n",
      "Epoch 25 loss: 37.06699653986769\n",
      "Epoch 26 loss: 36.8896484375\n",
      "Epoch 27 loss: 36.699866164693596\n",
      "Epoch 28 loss: 36.561719294045965\n",
      "Epoch 29 loss: 36.338236094185234\n",
      "Epoch 30 loss: 36.109013209435936\n",
      "Epoch 31 loss: 340.5932712395543\n",
      "Epoch 32 loss: 336.5729892061281\n",
      "Epoch 33 loss: 335.11797092618383\n",
      "Epoch 34 loss: 334.2958304317549\n",
      "Epoch 35 loss: 333.3009879874652\n",
      "Epoch 36 loss: 331.46537691504176\n",
      "Epoch 37 loss: 328.0242644498607\n",
      "Epoch 38 loss: 322.5931624303621\n",
      "Epoch 39 loss: 318.66728325208913\n",
      "Epoch 40 loss: 316.9158469707521\n",
      "Epoch 41 loss: 3126.6451949860725\n",
      "Epoch 42 loss: 3109.616991643454\n",
      "Epoch 43 loss: 3100.924442896936\n",
      "Epoch 44 loss: 3095.549791086351\n",
      "Epoch 45 loss: 3088.099582172702\n",
      "Epoch 46 loss: 3072.2778551532033\n",
      "Epoch 47 loss: 3044.991991643454\n",
      "Epoch 48 loss: 3032.859331476323\n",
      "Epoch 49 loss: 3023.800835654596\n",
      "Epoch 50 loss: 3015.6455431754875\n",
      "Epoch 51 loss: 30064.57103064067\n",
      "Epoch 52 loss: 29741.09192200557\n",
      "Epoch 53 loss: 29498.6713091922\n",
      "Epoch 54 loss: 29397.779944289694\n",
      "Epoch 55 loss: 29314.041782729804\n",
      "Epoch 56 loss: 29042.69080779944\n",
      "Epoch 57 loss: 28719.462395543174\n",
      "Epoch 58 loss: 28640.339832869082\n",
      "Epoch 59 loss: 28573.90250696379\n",
      "Epoch 60 loss: 28481.66852367688\n",
      "Epoch 61 loss: 283037.1922005571\n",
      "Epoch 62 loss: 279087.0640668524\n",
      "Epoch 63 loss: 274877.16991643456\n",
      "Epoch 64 loss: 268914.0278551532\n",
      "Epoch 65 loss: 267431.37604456826\n",
      "Epoch 66 loss: 265417.40389972145\n",
      "Epoch 67 loss: 264897.8495821727\n",
      "Epoch 68 loss: 264267.922005571\n",
      "Epoch 69 loss: 263389.9498607242\n",
      "Epoch 70 loss: 262823.7771587744\n",
      "Epoch 71 loss: 2613121.426183844\n",
      "Epoch 72 loss: 2604040.3788300836\n",
      "Epoch 73 loss: 2585076.7688022284\n",
      "Epoch 74 loss: 2580480.5348189417\n",
      "Epoch 75 loss: 2574751.91086351\n",
      "Epoch 76 loss: 2565967.8662952646\n",
      "Epoch 77 loss: 2557131.766016713\n",
      "Epoch 78 loss: 2548630.1058495822\n",
      "Epoch 79 loss: 2542647.9777158774\n",
      "Epoch 80 loss: 2535800.512534819\n",
      "Epoch 81 loss: 25172601.22562674\n",
      "Epoch 82 loss: 25074559.643454038\n",
      "Epoch 83 loss: 24887005.058495823\n",
      "Epoch 84 loss: 24753069.28133705\n",
      "Epoch 85 loss: 24689190.50696379\n",
      "Epoch 86 loss: 24532278.908077996\n",
      "Epoch 87 loss: 24381534.128133703\n",
      "Epoch 88 loss: 24248659.431754876\n",
      "Epoch 89 loss: 24070520.51253482\n",
      "Epoch 90 loss: 23981828.991643455\n",
      "Epoch 91 loss: 238068487.84401113\n",
      "Epoch 92 loss: 236284480.178273\n",
      "Epoch 93 loss: 233272471.17548746\n",
      "Epoch 94 loss: 231260707.6545961\n",
      "Epoch 95 loss: 229331776.8913649\n",
      "Epoch 96 loss: 227530609.3816156\n",
      "Epoch 97 loss: 226162659.47632313\n",
      "Epoch 98 loss: 224364891.98885792\n",
      "Epoch 99 loss: 223073705.0027855\n",
      "Epoch 100 loss: 221609162.51810583\n"
     ]
    }
   ],
   "source": [
    "metric_collector = MetricCollector()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in batches:\n",
    "        batch_loss = train_step(batch[0], batch[1], params)\n",
    "        epoch_loss += batch_loss\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss.numpy()/len(batches)}\")\n",
    "\n",
    "    # update stats\n",
    "    metric_collector.update_state(\n",
    "        tf.concat(state_train,0), \n",
    "        tf.concat(action_train,0), \n",
    "        tf.concat(state_test,0), \n",
    "        tf.concat(action_test,0), \n",
    "        params,\n",
    "    ) \n",
    "\n",
    "    # update parameters\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        params[\"lambda_collision\"] = params[\"lambda_collision\"] + params[\"eta_collision\"] * metric_collector.col['train'][-1]\n",
    "        params[\"mu_collision\"] = params[\"mu_collision\"] * params[\"beta_collision\"] \n",
    "        params[\"lambda_velocity\"] = params[\"lambda_velocity\"] + params[\"eta_velocity\"] * metric_collector.vel['train'][-1]\n",
    "        params[\"mu_velocity\"] = params[\"mu_velocity\"] * params[\"beta_velocity\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
