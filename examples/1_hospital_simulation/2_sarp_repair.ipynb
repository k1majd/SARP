{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sarp.utils import load_expert_data_hospital, separate_train_test, combine_nets, mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)],\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "currn_dir = os.path.dirname(os.path.abspath(\"2_sarp_repair.ipynb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of policy repair using SARP for the robot navigation in hospital. This script assumes that a pre-trained policy and a predictive model are already available. To pre-train a policy for this example run [0_pretrain_policy.py](0_pretrain_policy.py). Also to train a predictive model run [1_pretrain_predictive_model.py](1_pretrain_predictive_model.py). Here are the descriptions of models:\n",
    "- policy - input: the system state that includes the robot's goal, distancc and heading toward goal, and range sensor readings - output: linear and angular velocities.\n",
    "- predictive model - input: states and actions - output: collision [0, 1] or no collision [1, 0]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laod dataset\n",
    "First, we load the expert demonstrations for repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sample 1, goal: [10. 10.]\n",
      "loading sample 2, goal: [10. 10.]\n",
      "loading sample 3, goal: [10. 10.]\n",
      "loading sample 4, goal: [10. 10.]\n",
      "loading sample 5, goal: [10. 10.]\n",
      "loading sample 6, goal: [10. 10.]\n",
      "loading sample 7, goal: [10. 10.]\n",
      "loading sample 8, goal: [10. 10.]\n",
      "loading sample 9, goal: [10. 10.]\n",
      "loading sample 10, goal: [10. 10.]\n",
      "loading sample 11, goal: [-10.   5.]\n",
      "loading sample 12, goal: [-10.   5.]\n",
      "loading sample 13, goal: [-10.   5.]\n",
      "loading sample 14, goal: [-10.   5.]\n",
      "loading sample 15, goal: [-10.   5.]\n",
      "loading sample 16, goal: [-10.   5.]\n",
      "loading sample 17, goal: [-10.   5.]\n",
      "loading sample 18, goal: [-10.   5.]\n",
      "loading sample 19, goal: [-10.   5.]\n",
      "loading sample 20, goal: [-10.   5.]\n",
      "loading sample 21, goal: [10.  5.]\n",
      "loading sample 22, goal: [10.  5.]\n",
      "loading sample 23, goal: [10.  5.]\n",
      "loading sample 24, goal: [10.  5.]\n",
      "loading sample 25, goal: [10.  5.]\n",
      "loading sample 26, goal: [10.  5.]\n",
      "loading sample 27, goal: [10.  5.]\n",
      "loading sample 28, goal: [10.  5.]\n",
      "loading sample 29, goal: [10.  5.]\n",
      "loading sample 30, goal: [10.  5.]\n",
      "loading sample 31, goal: [-9. -9.]\n",
      "loading sample 32, goal: [-9. -9.]\n",
      "loading sample 33, goal: [-9. -9.]\n",
      "loading sample 34, goal: [-9. -9.]\n",
      "loading sample 35, goal: [-9. -9.]\n",
      "loading sample 36, goal: [-9. -9.]\n",
      "loading sample 37, goal: [-9. -9.]\n",
      "loading sample 38, goal: [-9. -9.]\n",
      "loading sample 39, goal: [-9. -9.]\n",
      "loading sample 40, goal: [-9. -9.]\n",
      "loading sample 41, goal: [ 9. -9.]\n",
      "loading sample 42, goal: [ 9. -9.]\n",
      "loading sample 43, goal: [ 9. -9.]\n",
      "loading sample 44, goal: [ 9. -9.]\n",
      "loading sample 45, goal: [ 9. -9.]\n",
      "loading sample 46, goal: [ 9. -9.]\n",
      "loading sample 47, goal: [ 9. -9.]\n",
      "loading sample 48, goal: [ 9. -9.]\n",
      "loading sample 49, goal: [ 9. -9.]\n",
      "loading sample 50, goal: [ 9. -9.]\n",
      "loading sample 51, goal: [-10.  10.]\n",
      "loading sample 52, goal: [-10.  10.]\n",
      "loading sample 53, goal: [-10.  10.]\n",
      "loading sample 54, goal: [-10.  10.]\n",
      "loading sample 55, goal: [-10.  10.]\n",
      "loading sample 56, goal: [-10.  10.]\n",
      "loading sample 57, goal: [-10.  10.]\n",
      "loading sample 58, goal: [-10.  10.]\n",
      "loading sample 59, goal: [-10.  10.]\n",
      "loading sample 60, goal: [-10.  10.]\n"
     ]
    }
   ],
   "source": [
    "# load the expert data\n",
    "data_dir = currn_dir + f\"/data/expert_data\"\n",
    "num_samples = len(os.listdir(data_dir))\n",
    "\n",
    "state, action, _, property = load_expert_data_hospital(data_dir, num_samples)\n",
    "state = [tf.convert_to_tensor(s, dtype=tf.float32) for s in state]\n",
    "action = [tf.convert_to_tensor(a, dtype=tf.float32) for a in action]\n",
    "property = [tf.convert_to_tensor(p, dtype=tf.float32) for p in property]\n",
    "train_data, test_data = separate_train_test([state, action, property], test_ratio=0.2)\n",
    "\n",
    "state_train, action_train, property_train = train_data\n",
    "state_test, action_test, property_test = test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and megre models\n",
    "Here, we load the policy and predictive models, then we merge them in a series fashion to be used in repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"repair_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " policy_layer_1 (Dense)      multiple                  3840      \n",
      "                                                                 \n",
      " policy_layer_2 (Dense)      multiple                  65792     \n",
      "                                                                 \n",
      " policy_layer_3 (Dense)      multiple                  514       \n",
      "                                                                 \n",
      " Predictive_layer_1 (Dense)  multiple                  4352      \n",
      "                                                                 \n",
      " Predictive_layer_2 (Dense)  multiple                  65792     \n",
      "                                                                 \n",
      " Predictive_layer_3 (Dense)  multiple                  2570      \n",
      "                                                                 \n",
      " Predictive_layer_4 (Dense)  multiple                  1408      \n",
      "                                                                 \n",
      " Predictive_layer_5 (Dense)  multiple                  16512     \n",
      "                                                                 \n",
      " Predictive_layer_6 (Dense)  multiple                  258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,038\n",
      "Trainable params: 161,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the models\n",
    "model_policy_orig = keras.models.load_model(\n",
    "    currn_dir\n",
    "    + f\"/trained_models/policy/model\"\n",
    "    )\n",
    "model_predictive = keras.models.load_model(\n",
    "    currn_dir\n",
    "    + f\"/trained_models/predictive_model/model\"\n",
    "    )\n",
    "\n",
    "# combine the models\n",
    "model_combined = combine_nets(model_policy_orig, model_predictive)\n",
    "\n",
    "# keep only the policy part of the combined model to be trained\n",
    "for layer in model_combined.layers:\n",
    "    if layer.name.split(\"_\")[0] == \"policy\":\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "_,_ = model_combined.predict(state[0][0:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimization parameters\n",
    "\n",
    "We first define the loss function, lagrangian penalty terms and the quadratic penalty terms. We assum two constraints:\n",
    "1. Constraint on linear velocity: $v\\leq0.9$\n",
    "2. Constraint on the output collision property: $\\psi = [1,0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "params = {\n",
    "    \"lambda_collision\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_collision\": tf.constant(5.0, dtype=\"float32\"),\n",
    "    \"eta_collision\": tf.constant(0.001, dtype=\"float32\"),\n",
    "    \"beta_collision\": tf.constant(5, dtype=\"float32\"),\n",
    "    \"lambda_velocity\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "    \"eta_velocity\": tf.constant(0.001, dtype=\"float32\"),\n",
    "    \"beta_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "}\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create data batches\n",
    "batches = mini_batch(\n",
    "    tf.concat(state_train,0),\n",
    "    tf.concat(action_train,0), \n",
    "    tf.concat(property_train,0), \n",
    "    batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def col_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(y[:, 1]))\n",
    "\n",
    "def col_lagrangian(y):\n",
    "    return tf.reduce_sum(y[:, 1])\n",
    "\n",
    "def vel_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(tf.nn.relu(y[:, 0] - 0.9)))\n",
    "\n",
    "def vel_lagrangian(y):\n",
    "    return tf.reduce_sum(tf.nn.relu(y[:, 0] - 0.9))\n",
    "\n",
    "def augmented_loss(\n",
    "    s, a, params\n",
    "):\n",
    "    a_pred, p_pred = model_combined(s)\n",
    "    loss_value = (\n",
    "            100 * original_loss(a, a_pred)\n",
    "            - params[\"lambda_collision\"] * col_lagrangian(p_pred)\n",
    "            + params[\"mu_collision\"] / 2 * col_penalty(p_pred)\n",
    "            - params[\"lambda_velocity\"] * vel_lagrangian(a_pred)\n",
    "            + params[\"mu_velocity\"] / 2 * vel_penalty(a_pred)\n",
    "        )\n",
    "    return (\n",
    "            loss_value,\n",
    "            original_loss(a, a_pred),\n",
    "            col_lagrangian(p_pred),\n",
    "            vel_lagrangian(a_pred),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the optimizer and the policy update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler():\n",
    "    def __init__(self, optimizer, lr_min=5e-5, decay=0.1, patience=10, loss_tol=0.0001):\n",
    "        self.lr_min = lr_min\n",
    "        self.patience = patience\n",
    "        self.decay = decay\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tol = loss_tol\n",
    "        self.counter = 0\n",
    "        self.loss_prev = 10000\n",
    "\n",
    "    def on_batch_end(self, loss):\n",
    "        if self.loss_prev - loss > self.loss_tol:\n",
    "            pass\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                self.counter = 0\n",
    "                new_lr = self.optimizer.learning_rate * self.decay\n",
    "                if new_lr.numpy() >= self.lr_min:\n",
    "                    self.optimizer.learning_rate.assign(new_lr)\n",
    "        \n",
    "        self.loss_prev = loss\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "lr_scheduler = LearningRateScheduler(optimizer, lr_min=5e-5, decay=0.1, patience=10, loss_tol=0.0001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    s, a, params\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value, _, _, _ = augmented_loss(\n",
    "            s, a, params\n",
    "        )\n",
    "    grads = tape.gradient(loss_value, model_combined.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_combined.trainable_variables))\n",
    "    return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair the policy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we repair the policy in the loop and adjust the lagrangian multiplier and penalty coefficient accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCollector():\n",
    "    def __init__(self):\n",
    "        self.loss = {'train':[], 'test':[]}\n",
    "        self.col = {'train':[], 'test':[]}\n",
    "        self.vel = {'train':[], 'test':[]}\n",
    "    \n",
    "    def update_state(self, s_train, a_train, s_test, a_test, params):\n",
    "        _, loss_train, col_train, vel_train = augmented_loss(\n",
    "            s_train, a_train, params\n",
    "        )\n",
    "        _, loss_test, col_test, vel_test = augmented_loss(\n",
    "            s_test, a_test, params\n",
    "        )\n",
    "        self.loss['train'].append(loss_train.numpy())\n",
    "        self.loss['test'].append(loss_test.numpy())\n",
    "        self.col['train'].append(col_train.numpy())\n",
    "        self.col['test'].append(col_test.numpy())\n",
    "        self.vel['train'].append(vel_train.numpy())\n",
    "        self.vel['test'].append(vel_test.numpy())\n",
    "\n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(3,1, figsize=(10,10))\n",
    "        ax[0].plot(self.loss['train'], label='train')\n",
    "        ax[0].plot(self.loss['test'], label='test')\n",
    "        ax[0].set_ylabel('loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(self.col['train'], label='train')\n",
    "        ax[1].plot(self.col['test'], label='test')\n",
    "        ax[1].set_ylabel('collision')\n",
    "        ax[1].legend()\n",
    "        ax[2].plot(self.vel['train'], label='train')\n",
    "        ax[2].plot(self.vel['test'], label='test')\n",
    "        ax[2].set_ylabel('velocity')\n",
    "        ax[2].legend()\n",
    "        plt.show()\n",
    "\n",
    "class Verbose():\n",
    "    def __init__(self, metric_collector, optimizer, epochs):\n",
    "        self.metric_collector = metric_collector\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.best_model = None\n",
    "\n",
    "    def on_batch_end(self, epoch, model):\n",
    "        print(f\"e: {epoch}/{self.epochs}, lr: {self.optimizer.learning_rate.numpy():.6f}, loss: {self.metric_collector.loss['train'][-1]:.4f}, col: {self.metric_collector.col['train'][-1]:.4f}, vel: {self.metric_collector.vel['train'][-1]:.4f}, loss_val: {self.metric_collector.loss['test'][-1]:.4f}, col_val: {self.metric_collector.col['test'][-1]:.4f}, vel_val: {self.metric_collector.vel['test'][-1]:.4f}\")\n",
    "        # keep the model with the best validation loss\n",
    "        #if self.metric_collector.col['test'][-1] == min(self.metric_collector.col['test']):\n",
    "            # use copy to avoid the model being overwritten\n",
    "            # self.best_model = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 1/100, lr: 0.001000, loss: 0.0265, col: 70.9803, vel: 289.8556, loss_val: 0.0305, col_val: 15.8997, vel_val: 79.8242\n",
      "e: 2/100, lr: 0.001000, loss: 0.0229, col: 69.1826, vel: 262.2209, loss_val: 0.0285, col_val: 16.2141, vel_val: 73.1083\n",
      "e: 3/100, lr: 0.001000, loss: 0.0217, col: 69.2616, vel: 235.4194, loss_val: 0.0272, col_val: 16.1283, vel_val: 72.1580\n",
      "e: 4/100, lr: 0.001000, loss: 0.0219, col: 67.9359, vel: 216.5017, loss_val: 0.0276, col_val: 16.1505, vel_val: 68.7797\n",
      "e: 5/100, lr: 0.001000, loss: 0.0223, col: 67.9558, vel: 228.9149, loss_val: 0.0287, col_val: 15.9976, vel_val: 65.4134\n",
      "e: 6/100, lr: 0.001000, loss: 0.0212, col: 68.3581, vel: 249.6495, loss_val: 0.0271, col_val: 16.2328, vel_val: 74.6559\n",
      "e: 7/100, lr: 0.001000, loss: 0.0214, col: 67.0797, vel: 332.0587, loss_val: 0.0276, col_val: 16.0138, vel_val: 90.5049\n",
      "e: 8/100, lr: 0.001000, loss: 0.0241, col: 66.6936, vel: 243.8947, loss_val: 0.0297, col_val: 16.1856, vel_val: 72.5476\n",
      "e: 9/100, lr: 0.001000, loss: 0.0214, col: 66.8393, vel: 266.1430, loss_val: 0.0270, col_val: 16.1089, vel_val: 71.9515\n",
      "e: 10/100, lr: 0.001000, loss: 0.0207, col: 65.6345, vel: 306.5533, loss_val: 0.0264, col_val: 15.9394, vel_val: 80.6418\n",
      "e: 11/100, lr: 0.001000, loss: 0.0269, col: 65.4831, vel: 36.5983, loss_val: 0.0322, col_val: 15.9472, vel_val: 11.7347\n",
      "e: 12/100, lr: 0.001000, loss: 0.0236, col: 62.0531, vel: 192.1946, loss_val: 0.0300, col_val: 15.9195, vel_val: 52.2456\n",
      "e: 13/100, lr: 0.001000, loss: 0.0232, col: 61.6633, vel: 165.1022, loss_val: 0.0294, col_val: 15.6117, vel_val: 47.4056\n",
      "e: 14/100, lr: 0.001000, loss: 0.0234, col: 60.8554, vel: 104.6559, loss_val: 0.0297, col_val: 15.9646, vel_val: 28.6058\n",
      "e: 15/100, lr: 0.001000, loss: 0.0216, col: 59.8798, vel: 279.7703, loss_val: 0.0279, col_val: 15.9533, vel_val: 78.9567\n",
      "e: 16/100, lr: 0.001000, loss: 0.0227, col: 60.4220, vel: 108.4129, loss_val: 0.0284, col_val: 16.1228, vel_val: 29.9518\n",
      "e: 17/100, lr: 0.001000, loss: 0.0220, col: 60.9817, vel: 137.7835, loss_val: 0.0293, col_val: 15.9528, vel_val: 38.3647\n",
      "e: 18/100, lr: 0.001000, loss: 0.0222, col: 59.7817, vel: 207.2515, loss_val: 0.0285, col_val: 16.0244, vel_val: 54.4080\n",
      "e: 19/100, lr: 0.001000, loss: 0.0228, col: 60.9928, vel: 79.9257, loss_val: 0.0300, col_val: 16.1003, vel_val: 20.1919\n",
      "e: 20/100, lr: 0.001000, loss: 0.0245, col: 61.8542, vel: 157.0720, loss_val: 0.0300, col_val: 15.9309, vel_val: 40.0710\n",
      "e: 21/100, lr: 0.001000, loss: 0.0374, col: 62.6579, vel: 16.6617, loss_val: 0.0392, col_val: 15.8217, vel_val: 3.3428\n",
      "e: 22/100, lr: 0.001000, loss: 0.0329, col: 60.7889, vel: 15.3129, loss_val: 0.0370, col_val: 16.1419, vel_val: 2.1800\n",
      "e: 23/100, lr: 0.001000, loss: 0.0272, col: 58.2346, vel: 48.1362, loss_val: 0.0333, col_val: 16.0771, vel_val: 10.5596\n",
      "e: 24/100, lr: 0.001000, loss: 0.0283, col: 57.5396, vel: 26.1884, loss_val: 0.0328, col_val: 15.7505, vel_val: 6.4623\n",
      "e: 25/100, lr: 0.001000, loss: 0.0260, col: 57.2116, vel: 35.2936, loss_val: 0.0336, col_val: 14.5927, vel_val: 7.7914\n",
      "e: 26/100, lr: 0.001000, loss: 0.0259, col: 56.1780, vel: 49.0874, loss_val: 0.0329, col_val: 15.0652, vel_val: 12.9577\n",
      "e: 27/100, lr: 0.000100, loss: 0.0236, col: 55.1822, vel: 36.9219, loss_val: 0.0317, col_val: 15.1365, vel_val: 8.6180\n",
      "e: 28/100, lr: 0.000100, loss: 0.0230, col: 54.8040, vel: 38.4149, loss_val: 0.0316, col_val: 15.2739, vel_val: 8.8126\n",
      "e: 29/100, lr: 0.000100, loss: 0.0228, col: 54.2983, vel: 42.7558, loss_val: 0.0321, col_val: 15.2971, vel_val: 9.6205\n",
      "e: 30/100, lr: 0.000100, loss: 0.0230, col: 53.3996, vel: 44.2219, loss_val: 0.0325, col_val: 15.3275, vel_val: 9.6195\n",
      "e: 31/100, lr: 0.000100, loss: 0.0252, col: 51.2384, vel: 15.1972, loss_val: 0.0352, col_val: 14.9824, vel_val: 2.3472\n",
      "e: 32/100, lr: 0.000100, loss: 0.0260, col: 49.3138, vel: 14.5890, loss_val: 0.0371, col_val: 14.7636, vel_val: 2.3953\n",
      "e: 33/100, lr: 0.000100, loss: 0.0270, col: 47.7959, vel: 14.8711, loss_val: 0.0384, col_val: 14.6652, vel_val: 2.4149\n",
      "e: 34/100, lr: 0.000100, loss: 0.0272, col: 47.5083, vel: 13.8825, loss_val: 0.0385, col_val: 14.5053, vel_val: 2.2859\n",
      "e: 35/100, lr: 0.000100, loss: 0.0271, col: 47.5730, vel: 12.5597, loss_val: 0.0381, col_val: 14.4015, vel_val: 2.0472\n",
      "e: 36/100, lr: 0.000100, loss: 0.0272, col: 47.4017, vel: 13.4729, loss_val: 0.0382, col_val: 13.8712, vel_val: 2.2320\n",
      "e: 37/100, lr: 0.000100, loss: 0.0272, col: 47.5369, vel: 12.6624, loss_val: 0.0379, col_val: 13.7860, vel_val: 1.9935\n",
      "e: 38/100, lr: 0.000100, loss: 0.0271, col: 47.2213, vel: 12.2577, loss_val: 0.0386, col_val: 13.2714, vel_val: 1.9304\n",
      "e: 39/100, lr: 0.000100, loss: 0.0269, col: 47.1572, vel: 11.4334, loss_val: 0.0379, col_val: 12.9852, vel_val: 1.7813\n",
      "e: 40/100, lr: 0.000100, loss: 0.0267, col: 47.1582, vel: 12.0573, loss_val: 0.0377, col_val: 12.8000, vel_val: 1.8901\n",
      "e: 41/100, lr: 0.000100, loss: 0.0298, col: 46.7445, vel: 9.1636, loss_val: 0.0426, col_val: 12.2283, vel_val: 1.2024\n",
      "e: 42/100, lr: 0.000100, loss: 0.0296, col: 46.6941, vel: 7.4171, loss_val: 0.0424, col_val: 11.8761, vel_val: 0.7334\n",
      "e: 43/100, lr: 0.000100, loss: 0.0291, col: 46.2285, vel: 9.1709, loss_val: 0.0417, col_val: 11.7851, vel_val: 1.3011\n",
      "e: 44/100, lr: 0.000100, loss: 0.0303, col: 45.5704, vel: 11.9183, loss_val: 0.0441, col_val: 11.1089, vel_val: 2.0711\n",
      "e: 45/100, lr: 0.000100, loss: 0.0305, col: 45.4154, vel: 9.5034, loss_val: 0.0435, col_val: 11.0395, vel_val: 1.4511\n",
      "e: 46/100, lr: 0.000100, loss: 0.0312, col: 45.4143, vel: 8.3921, loss_val: 0.0449, col_val: 10.9181, vel_val: 1.1176\n",
      "e: 47/100, lr: 0.000100, loss: 0.0313, col: 45.3587, vel: 8.1551, loss_val: 0.0449, col_val: 11.0766, vel_val: 1.1477\n",
      "e: 48/100, lr: 0.000100, loss: 0.0310, col: 45.0368, vel: 9.3128, loss_val: 0.0443, col_val: 10.9433, vel_val: 1.3118\n",
      "e: 49/100, lr: 0.000100, loss: 0.0321, col: 45.5785, vel: 7.4932, loss_val: 0.0452, col_val: 10.9991, vel_val: 0.9768\n",
      "e: 50/100, lr: 0.000100, loss: 0.0303, col: 45.1320, vel: 8.4710, loss_val: 0.0429, col_val: 11.0303, vel_val: 1.1864\n",
      "e: 51/100, lr: 0.000100, loss: 0.0334, col: 44.5658, vel: 10.6557, loss_val: 0.0479, col_val: 10.4695, vel_val: 1.7574\n",
      "e: 52/100, lr: 0.000100, loss: 0.0325, col: 44.3352, vel: 7.4938, loss_val: 0.0454, col_val: 10.9456, vel_val: 0.8786\n",
      "e: 53/100, lr: 0.000100, loss: 0.0323, col: 44.1413, vel: 7.9280, loss_val: 0.0457, col_val: 11.1035, vel_val: 1.1284\n",
      "e: 54/100, lr: 0.000100, loss: 0.0328, col: 43.5434, vel: 8.8222, loss_val: 0.0463, col_val: 10.8980, vel_val: 1.2612\n",
      "e: 55/100, lr: 0.000100, loss: 0.0342, col: 43.6780, vel: 7.2392, loss_val: 0.0472, col_val: 11.1579, vel_val: 0.9060\n",
      "e: 56/100, lr: 0.000100, loss: 0.0348, col: 43.1838, vel: 8.6363, loss_val: 0.0481, col_val: 10.7544, vel_val: 1.3859\n",
      "e: 57/100, lr: 0.000100, loss: 0.0330, col: 43.6742, vel: 7.0702, loss_val: 0.0454, col_val: 11.0705, vel_val: 1.0882\n",
      "e: 58/100, lr: 0.000100, loss: 0.0342, col: 43.0728, vel: 7.6517, loss_val: 0.0468, col_val: 11.0418, vel_val: 0.8778\n",
      "e: 59/100, lr: 0.000100, loss: 0.0339, col: 43.6270, vel: 5.9912, loss_val: 0.0473, col_val: 11.0534, vel_val: 0.8660\n",
      "e: 60/100, lr: 0.000100, loss: 0.0337, col: 42.9569, vel: 7.3883, loss_val: 0.0471, col_val: 11.0305, vel_val: 0.9696\n",
      "e: 61/100, lr: 0.000100, loss: 0.0372, col: 42.9730, vel: 9.2001, loss_val: 0.0511, col_val: 10.7239, vel_val: 1.8932\n",
      "e: 62/100, lr: 0.000100, loss: 0.0386, col: 43.0683, vel: 6.7271, loss_val: 0.0506, col_val: 10.8914, vel_val: 0.7611\n",
      "e: 63/100, lr: 0.000100, loss: 0.0389, col: 42.6398, vel: 6.6560, loss_val: 0.0506, col_val: 10.6811, vel_val: 0.8740\n",
      "e: 64/100, lr: 0.000100, loss: 0.0378, col: 42.8529, vel: 6.8317, loss_val: 0.0488, col_val: 10.6887, vel_val: 1.0212\n",
      "e: 65/100, lr: 0.000100, loss: 0.0395, col: 42.5785, vel: 6.4447, loss_val: 0.0521, col_val: 10.7487, vel_val: 0.6162\n",
      "e: 66/100, lr: 0.000100, loss: 0.0377, col: 42.7622, vel: 6.2615, loss_val: 0.0490, col_val: 10.5791, vel_val: 0.8903\n",
      "e: 67/100, lr: 0.000100, loss: 0.0394, col: 42.2778, vel: 6.7561, loss_val: 0.0521, col_val: 10.6975, vel_val: 0.7575\n",
      "e: 68/100, lr: 0.000100, loss: 0.0388, col: 41.7147, vel: 7.1913, loss_val: 0.0508, col_val: 10.7129, vel_val: 0.9660\n",
      "e: 69/100, lr: 0.000100, loss: 0.0399, col: 42.1102, vel: 6.2949, loss_val: 0.0517, col_val: 10.5634, vel_val: 1.2039\n",
      "e: 70/100, lr: 0.000100, loss: 0.0399, col: 41.4534, vel: 8.4849, loss_val: 0.0521, col_val: 10.5906, vel_val: 1.2545\n",
      "e: 71/100, lr: 0.000100, loss: 0.0465, col: 41.7300, vel: 9.9347, loss_val: 0.0595, col_val: 10.3250, vel_val: 1.6860\n",
      "e: 72/100, lr: 0.000100, loss: 0.0427, col: 41.7232, vel: 7.7166, loss_val: 0.0519, col_val: 10.6446, vel_val: 1.2634\n",
      "e: 73/100, lr: 0.000100, loss: 0.0439, col: 40.9578, vel: 7.8544, loss_val: 0.0549, col_val: 10.7698, vel_val: 0.9415\n",
      "e: 74/100, lr: 0.000100, loss: 0.0449, col: 41.4224, vel: 5.9665, loss_val: 0.0563, col_val: 10.5840, vel_val: 0.7850\n",
      "e: 75/100, lr: 0.000100, loss: 0.0437, col: 40.9829, vel: 7.5218, loss_val: 0.0538, col_val: 10.4453, vel_val: 1.0583\n",
      "e: 76/100, lr: 0.000100, loss: 0.0451, col: 40.9113, vel: 6.7189, loss_val: 0.0562, col_val: 10.5075, vel_val: 0.8837\n",
      "e: 77/100, lr: 0.000100, loss: 0.0443, col: 40.9943, vel: 6.9054, loss_val: 0.0548, col_val: 10.3325, vel_val: 1.1302\n",
      "e: 78/100, lr: 0.000100, loss: 0.0451, col: 41.2343, vel: 5.9618, loss_val: 0.0553, col_val: 10.4251, vel_val: 0.8907\n",
      "e: 79/100, lr: 0.000100, loss: 0.0436, col: 40.6899, vel: 7.4366, loss_val: 0.0540, col_val: 10.3371, vel_val: 1.2228\n",
      "e: 80/100, lr: 0.000100, loss: 0.0463, col: 41.2468, vel: 6.1078, loss_val: 0.0577, col_val: 10.3326, vel_val: 1.1686\n",
      "e: 81/100, lr: 0.000100, loss: 0.0442, col: 41.5832, vel: 7.5398, loss_val: 0.0550, col_val: 10.2440, vel_val: 1.6862\n",
      "e: 82/100, lr: 0.000100, loss: 0.0493, col: 41.2441, vel: 7.6838, loss_val: 0.0639, col_val: 10.6127, vel_val: 1.6210\n",
      "e: 83/100, lr: 0.000100, loss: 0.0473, col: 40.4453, vel: 7.4990, loss_val: 0.0586, col_val: 10.4144, vel_val: 1.6932\n",
      "e: 84/100, lr: 0.000100, loss: 0.0483, col: 41.5824, vel: 4.8908, loss_val: 0.0591, col_val: 10.6982, vel_val: 1.1901\n",
      "e: 85/100, lr: 0.000100, loss: 0.0477, col: 40.4985, vel: 7.2316, loss_val: 0.0593, col_val: 10.5252, vel_val: 1.4558\n",
      "e: 86/100, lr: 0.000100, loss: 0.0485, col: 40.8773, vel: 5.7981, loss_val: 0.0607, col_val: 10.4428, vel_val: 1.5215\n",
      "e: 87/100, lr: 0.000100, loss: 0.0485, col: 40.2964, vel: 7.9222, loss_val: 0.0611, col_val: 10.1950, vel_val: 1.8847\n",
      "e: 88/100, lr: 0.000100, loss: 0.0489, col: 40.7528, vel: 6.3708, loss_val: 0.0607, col_val: 10.1842, vel_val: 1.4154\n",
      "e: 89/100, lr: 0.000100, loss: 0.0494, col: 40.3038, vel: 7.2410, loss_val: 0.0623, col_val: 10.3100, vel_val: 1.4166\n",
      "e: 90/100, lr: 0.000100, loss: 0.0499, col: 40.5550, vel: 6.6204, loss_val: 0.0620, col_val: 10.1540, vel_val: 1.5021\n",
      "e: 91/100, lr: 0.000100, loss: 0.0528, col: 40.7644, vel: 9.5485, loss_val: 0.0691, col_val: 9.8187, vel_val: 2.2266\n",
      "e: 92/100, lr: 0.000100, loss: 0.0486, col: 41.5424, vel: 6.7961, loss_val: 0.0601, col_val: 10.0617, vel_val: 1.8751\n",
      "e: 93/100, lr: 0.000100, loss: 0.0500, col: 39.8788, vel: 8.6173, loss_val: 0.0635, col_val: 10.3713, vel_val: 1.3908\n",
      "e: 94/100, lr: 0.000100, loss: 0.0516, col: 40.5827, vel: 5.9683, loss_val: 0.0643, col_val: 10.3560, vel_val: 1.2388\n",
      "e: 95/100, lr: 0.000100, loss: 0.0517, col: 39.6678, vel: 8.0239, loss_val: 0.0650, col_val: 10.0727, vel_val: 1.8182\n",
      "e: 96/100, lr: 0.000100, loss: 0.0513, col: 40.3888, vel: 6.3298, loss_val: 0.0639, col_val: 10.1764, vel_val: 1.2659\n",
      "e: 97/100, lr: 0.000100, loss: 0.0532, col: 39.8682, vel: 7.5238, loss_val: 0.0660, col_val: 10.1232, vel_val: 1.6457\n",
      "e: 98/100, lr: 0.000100, loss: 0.0519, col: 39.9215, vel: 6.9572, loss_val: 0.0648, col_val: 10.1147, vel_val: 1.6556\n",
      "e: 99/100, lr: 0.000100, loss: 0.0521, col: 39.5195, vel: 7.8388, loss_val: 0.0645, col_val: 10.1533, vel_val: 1.6254\n",
      "e: 100/100, lr: 0.000100, loss: 0.0526, col: 39.8376, vel: 7.4099, loss_val: 0.0659, col_val: 10.0434, vel_val: 1.5837\n"
     ]
    }
   ],
   "source": [
    "metric_collector = MetricCollector()\n",
    "verbose = Verbose(metric_collector, optimizer, epochs)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in batches:\n",
    "        batch_loss = train_step(batch[0], batch[1], params)\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "    # update stats\n",
    "    metric_collector.update_state(\n",
    "        tf.concat(state_train,0), \n",
    "        tf.concat(action_train,0), \n",
    "        tf.concat(state_test,0), \n",
    "        tf.concat(action_test,0), \n",
    "        params,\n",
    "    ) \n",
    "\n",
    "    # update parameters\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        params[\"lambda_collision\"] = params[\"lambda_collision\"] + params[\"eta_collision\"] * metric_collector.col['train'][-1]\n",
    "        params[\"mu_collision\"] = params[\"mu_collision\"] * params[\"beta_collision\"] \n",
    "        params[\"lambda_velocity\"] = params[\"lambda_velocity\"] + params[\"eta_velocity\"] * metric_collector.vel['train'][-1]\n",
    "        params[\"mu_velocity\"] = params[\"mu_velocity\"] * params[\"beta_velocity\"]\n",
    "    \n",
    "    # print stats\n",
    "    verbose.on_batch_end(epoch+1, model_combined)\n",
    "    \n",
    "    # update learning rate\n",
    "    lr_scheduler.on_batch_end(metric_collector.col['test'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
