{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from sarp.utils import load_expert_data_hospital, separate_train_test, combine_nets, mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)],\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"2_sarp_repair.ipynb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example of policy repair using SARP for the robot navigation in hospital. This script assumes that a pre-trained policy and a predictive model are already available. To pre-train a policy for this example run [0_pretrain_policy.py](0_pretrain_policy.py). Also to train a predictive model run [1_pretrain_predictive_model.py](1_pretrain_predictive_model.py). Here are the descriptions of models:\n",
    "- policy - input: the system state that includes the robot's goal, distancc and heading toward goal, and range sensor readings - output: linear and angular velocities.\n",
    "- predictive model - input: states and actions - output: collision [0, 1] or no collision [1, 0]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laod dataset\n",
    "First, we load the expert demonstrations for repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sample 1, goal: [10. 10.]\n",
      "loading sample 2, goal: [10. 10.]\n",
      "loading sample 3, goal: [10. 10.]\n",
      "loading sample 4, goal: [10. 10.]\n",
      "loading sample 5, goal: [10. 10.]\n",
      "loading sample 6, goal: [10. 10.]\n",
      "loading sample 7, goal: [10. 10.]\n",
      "loading sample 8, goal: [10. 10.]\n",
      "loading sample 9, goal: [10. 10.]\n",
      "loading sample 10, goal: [10. 10.]\n",
      "loading sample 11, goal: [-10.   5.]\n",
      "loading sample 12, goal: [-10.   5.]\n",
      "loading sample 13, goal: [-10.   5.]\n",
      "loading sample 14, goal: [-10.   5.]\n",
      "loading sample 15, goal: [-10.   5.]\n",
      "loading sample 16, goal: [-10.   5.]\n",
      "loading sample 17, goal: [-10.   5.]\n",
      "loading sample 18, goal: [-10.   5.]\n",
      "loading sample 19, goal: [-10.   5.]\n",
      "loading sample 20, goal: [-10.   5.]\n",
      "loading sample 21, goal: [10.  5.]\n",
      "loading sample 22, goal: [10.  5.]\n",
      "loading sample 23, goal: [10.  5.]\n",
      "loading sample 24, goal: [10.  5.]\n",
      "loading sample 25, goal: [10.  5.]\n",
      "loading sample 26, goal: [10.  5.]\n",
      "loading sample 27, goal: [10.  5.]\n",
      "loading sample 28, goal: [10.  5.]\n",
      "loading sample 29, goal: [10.  5.]\n",
      "loading sample 30, goal: [10.  5.]\n",
      "loading sample 31, goal: [-9. -9.]\n",
      "loading sample 32, goal: [-9. -9.]\n",
      "loading sample 33, goal: [-9. -9.]\n",
      "loading sample 34, goal: [-9. -9.]\n",
      "loading sample 35, goal: [-9. -9.]\n",
      "loading sample 36, goal: [-9. -9.]\n",
      "loading sample 37, goal: [-9. -9.]\n",
      "loading sample 38, goal: [-9. -9.]\n",
      "loading sample 39, goal: [-9. -9.]\n",
      "loading sample 40, goal: [-9. -9.]\n",
      "loading sample 41, goal: [ 9. -9.]\n",
      "loading sample 42, goal: [ 9. -9.]\n",
      "loading sample 43, goal: [ 9. -9.]\n",
      "loading sample 44, goal: [ 9. -9.]\n",
      "loading sample 45, goal: [ 9. -9.]\n",
      "loading sample 46, goal: [ 9. -9.]\n",
      "loading sample 47, goal: [ 9. -9.]\n",
      "loading sample 48, goal: [ 9. -9.]\n",
      "loading sample 49, goal: [ 9. -9.]\n",
      "loading sample 50, goal: [ 9. -9.]\n",
      "loading sample 51, goal: [-10.  10.]\n",
      "loading sample 52, goal: [-10.  10.]\n",
      "loading sample 53, goal: [-10.  10.]\n",
      "loading sample 54, goal: [-10.  10.]\n",
      "loading sample 55, goal: [-10.  10.]\n",
      "loading sample 56, goal: [-10.  10.]\n",
      "loading sample 57, goal: [-10.  10.]\n",
      "loading sample 58, goal: [-10.  10.]\n",
      "loading sample 59, goal: [-10.  10.]\n",
      "loading sample 60, goal: [-10.  10.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 11:49:58.756900: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-08 11:49:59.342580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5120 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:d5:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# load the expert data\n",
    "data_dir = current_dir + f\"/data/expert_data\"\n",
    "num_samples = len(os.listdir(data_dir))\n",
    "\n",
    "state, action, _, property = load_expert_data_hospital(data_dir, num_samples, col_remove=True)\n",
    "state = [tf.convert_to_tensor(s, dtype=tf.float32) for s in state]\n",
    "action = [tf.convert_to_tensor(a, dtype=tf.float32) for a in action]\n",
    "property = [tf.convert_to_tensor(p, dtype=tf.float32) for p in property]\n",
    "train_data, test_data = separate_train_test([state, action, property], test_ratio=0.2)\n",
    "\n",
    "state_train, action_train, property_train = train_data\n",
    "state_test, action_test, property_test = test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and megre models\n",
    "Here, we load the policy and predictive models, then we merge them in a series fashion to be used in repair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"repair_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " policy_layer_1 (Dense)      multiple                  3840      \n",
      "                                                                 \n",
      " policy_layer_2 (Dense)      multiple                  65792     \n",
      "                                                                 \n",
      " policy_layer_3 (Dense)      multiple                  514       \n",
      "                                                                 \n",
      " Predictive_layer_1 (Dense)  multiple                  4352      \n",
      "                                                                 \n",
      " Predictive_layer_2 (Dense)  multiple                  65792     \n",
      "                                                                 \n",
      " Predictive_layer_3 (Dense)  multiple                  2570      \n",
      "                                                                 \n",
      " Predictive_layer_4 (Dense)  multiple                  1408      \n",
      "                                                                 \n",
      " Predictive_layer_5 (Dense)  multiple                  16512     \n",
      "                                                                 \n",
      " Predictive_layer_6 (Dense)  multiple                  258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,038\n",
      "Trainable params: 161,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the models\n",
    "model_policy_orig = keras.models.load_model(\n",
    "    current_dir\n",
    "    + f\"/trained_models/policy/model\"\n",
    "    )\n",
    "model_predictive = keras.models.load_model(\n",
    "    current_dir\n",
    "    + f\"/trained_models/predictive_model/model\"\n",
    "    )\n",
    "\n",
    "# combine the models\n",
    "model_combined = combine_nets(\n",
    "    model_policy_orig, \n",
    "    model_predictive, \n",
    "    state_indices_passed=[\n",
    "        state_train[0].shape[1]-i for i in range(\n",
    "            state_train[0].shape[1], 0, -1\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# keep only the policy part of the combined model to be trained\n",
    "for layer in model_combined.layers:\n",
    "    if layer.name.split(\"_\")[0] == \"policy\":\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "_,_ = model_combined.predict(state[0][0:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimization parameters\n",
    "\n",
    "We first define the loss function, lagrangian penalty terms and the quadratic penalty terms. We assum two constraints:\n",
    "1. Constraint on linear velocity: $v\\leq0.9$         $\\Longrightarrow g_{vel} = ReLU(v-0.9)$\n",
    "2. Constraint on the output collision property: $\\psi = [1,0]$     $\\Longrightarrow g_{col} = \\psi[1]$\n",
    "\n",
    "The augmented loss is formulated as \n",
    "\n",
    "\\begin{align} \n",
    "\\mathcal{L}^a =  \\mathcal{L}_{original} -\\lambda_{col} g_{col} + \\frac{\\mu_{col}}{2}g^2_{col}-\\lambda_{vel} g_{vel} + \\frac{\\mu_{vel}}{2}g^2_{vel}\\nonumber\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "params = {\n",
    "    \"lambda_collision\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_collision\": tf.constant(10.0, dtype=\"float32\"),\n",
    "    \"eta_collision\": tf.constant(0.001, dtype=\"float32\"),\n",
    "    \"beta_collision\": tf.constant(5, dtype=\"float32\"),\n",
    "    \"lambda_velocity\": tf.constant(0.0, dtype=\"float32\"),\n",
    "    \"mu_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "    \"eta_velocity\": tf.constant(0.001, dtype=\"float32\"),\n",
    "    \"beta_velocity\": tf.constant(5.0, dtype=\"float32\"),\n",
    "}\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create data batches\n",
    "batches = mini_batch(\n",
    "    tf.concat(state_train,0),\n",
    "    tf.concat(action_train,0), \n",
    "    tf.concat(property_train,0), \n",
    "    batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def col_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(y[:, 1]))\n",
    "\n",
    "def col_lagrangian(y):\n",
    "    return tf.reduce_sum(y[:, 1])\n",
    "\n",
    "def vel_penalty(y):\n",
    "    return tf.reduce_sum(tf.square(tf.nn.relu(y[:, 0] - 0.9)))\n",
    "\n",
    "def vel_lagrangian(y):\n",
    "    return tf.reduce_sum(tf.nn.relu(y[:, 0] - 0.9))\n",
    "\n",
    "def augmented_loss(\n",
    "    s, a, params\n",
    "):\n",
    "    a_pred, p_pred = model_combined(s)\n",
    "    loss_value = (\n",
    "            100 * original_loss(a, a_pred)\n",
    "            - params[\"lambda_collision\"] * col_lagrangian(p_pred)\n",
    "            + params[\"mu_collision\"] / 2 * col_penalty(p_pred)\n",
    "            - params[\"lambda_velocity\"] * vel_lagrangian(a_pred)\n",
    "            + params[\"mu_velocity\"] / 2 * vel_penalty(a_pred)\n",
    "        )\n",
    "    return (\n",
    "            loss_value,\n",
    "            original_loss(a, a_pred),\n",
    "            col_lagrangian(p_pred),\n",
    "            vel_lagrangian(a_pred),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the optimizer and the policy update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler():\n",
    "    def __init__(self, optimizer, lr_min=5e-5, decay=0.1, patience=10, loss_tol=0.0001):\n",
    "        self.lr_min = lr_min\n",
    "        self.patience = patience\n",
    "        self.decay = decay\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tol = loss_tol\n",
    "        self.counter = 0\n",
    "        self.loss_prev = 10000\n",
    "\n",
    "    def on_batch_end(self, loss):\n",
    "        if self.loss_prev - loss > self.loss_tol:\n",
    "            pass\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                self.counter = 0\n",
    "                new_lr = self.optimizer.learning_rate * self.decay\n",
    "                if new_lr.numpy() >= self.lr_min:\n",
    "                    self.optimizer.learning_rate.assign(new_lr)\n",
    "        \n",
    "        self.loss_prev = loss\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "lr_scheduler = LearningRateScheduler(optimizer, lr_min=5e-5, decay=0.1, patience=10, loss_tol=0.0001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    s, a, params\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value, _, _, _ = augmented_loss(\n",
    "            s, a, params\n",
    "        )\n",
    "    grads = tape.gradient(loss_value, model_combined.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_combined.trainable_variables))\n",
    "    return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repair the policy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we repair the policy in the loop and adjust the lagrangian multiplier and penalty coefficient accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCollector():\n",
    "    def __init__(self):\n",
    "        self.loss = {'train':[], 'test':[]}\n",
    "        self.col = {'train':[], 'test':[]}\n",
    "        self.vel = {'train':[], 'test':[]}\n",
    "        self.best_weights = []\n",
    "    \n",
    "    def update_state(self, s_train, a_train, s_test, a_test, params):\n",
    "        _, loss_train, col_train, vel_train = augmented_loss(\n",
    "            s_train, a_train, params\n",
    "        )\n",
    "        _, loss_test, col_test, vel_test = augmented_loss(\n",
    "            s_test, a_test, params\n",
    "        )\n",
    "        self.loss['train'].append(loss_train.numpy())\n",
    "        self.loss['test'].append(loss_test.numpy())\n",
    "        self.col['train'].append(col_train.numpy())\n",
    "        self.col['test'].append(col_test.numpy())\n",
    "        self.vel['train'].append(vel_train.numpy())\n",
    "        self.vel['test'].append(vel_test.numpy())\n",
    "    \n",
    "    def save_best_model(self, model):\n",
    "        if self.col['test'][-1] == min(self.col['test']):\n",
    "            self.best_weights = model.get_weights()[:(len(model.policy_arch)-1)*2]\n",
    "\n",
    "    def plot(self):\n",
    "        _, ax = plt.subplots(3,1, figsize=(10,10))\n",
    "        ax[0].plot(self.loss['train'], label='train')\n",
    "        ax[0].plot(self.loss['test'], label='test')\n",
    "        ax[0].set_ylabel('loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(self.col['train'], label='train')\n",
    "        ax[1].plot(self.col['test'], label='test')\n",
    "        ax[1].set_ylabel('collision')\n",
    "        ax[1].legend()\n",
    "        ax[2].plot(self.vel['train'], label='train')\n",
    "        ax[2].plot(self.vel['test'], label='test')\n",
    "        ax[2].set_ylabel('velocity')\n",
    "        ax[2].legend()\n",
    "        plt.show()\n",
    "\n",
    "class Verbose():\n",
    "    def __init__(self, metric_collector, optimizer, epochs):\n",
    "        self.metric_collector = metric_collector\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.best_model = None\n",
    "\n",
    "    def on_batch_end(self, epoch, model):\n",
    "        print(f\"e: {epoch}/{self.epochs}, lr: {self.optimizer.learning_rate.numpy():.6f}, loss: {self.metric_collector.loss['train'][-1]:.4f}, col: {self.metric_collector.col['train'][-1]:.4f}, vel: {self.metric_collector.vel['train'][-1]:.4f}, loss_val: {self.metric_collector.loss['test'][-1]:.4f}, col_val: {self.metric_collector.col['test'][-1]:.4f}, vel_val: {self.metric_collector.vel['test'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 1/100, lr: 0.001000, loss: 0.0224, col: 22.6735, vel: 373.4587, loss_val: 0.0298, col_val: 9.1333, vel_val: 78.9730\n",
      "e: 2/100, lr: 0.001000, loss: 0.0232, col: 21.4083, vel: 325.6610, loss_val: 0.0295, col_val: 9.1373, vel_val: 72.6909\n",
      "e: 3/100, lr: 0.001000, loss: 0.0232, col: 19.9839, vel: 282.3079, loss_val: 0.0315, col_val: 9.1917, vel_val: 65.3221\n",
      "e: 4/100, lr: 0.001000, loss: 0.0240, col: 19.4262, vel: 323.0574, loss_val: 0.0312, col_val: 9.0611, vel_val: 72.4234\n",
      "e: 5/100, lr: 0.001000, loss: 0.0225, col: 19.0552, vel: 303.4448, loss_val: 0.0329, col_val: 9.3303, vel_val: 70.3033\n",
      "e: 6/100, lr: 0.001000, loss: 0.0226, col: 20.1797, vel: 323.0415, loss_val: 0.0304, col_val: 9.2217, vel_val: 76.3078\n",
      "e: 7/100, lr: 0.001000, loss: 0.0219, col: 18.6406, vel: 323.0372, loss_val: 0.0314, col_val: 9.0287, vel_val: 71.0497\n",
      "e: 8/100, lr: 0.001000, loss: 0.0232, col: 19.3568, vel: 330.9801, loss_val: 0.0333, col_val: 9.1561, vel_val: 71.4689\n",
      "e: 9/100, lr: 0.001000, loss: 0.0242, col: 19.6495, vel: 308.1490, loss_val: 0.0349, col_val: 8.7865, vel_val: 72.6540\n",
      "e: 10/100, lr: 0.001000, loss: 0.0226, col: 20.2504, vel: 384.6895, loss_val: 0.0325, col_val: 9.2684, vel_val: 83.5005\n",
      "e: 11/100, lr: 0.001000, loss: 0.0289, col: 14.5372, vel: 124.7700, loss_val: 0.0355, col_val: 8.7784, vel_val: 29.0313\n",
      "e: 12/100, lr: 0.001000, loss: 0.0246, col: 17.0789, vel: 125.1279, loss_val: 0.0323, col_val: 9.2201, vel_val: 26.2327\n",
      "e: 13/100, lr: 0.001000, loss: 0.0226, col: 15.6131, vel: 207.0376, loss_val: 0.0318, col_val: 9.0123, vel_val: 44.5340\n",
      "e: 14/100, lr: 0.001000, loss: 0.0247, col: 17.5150, vel: 241.8213, loss_val: 0.0326, col_val: 9.1901, vel_val: 56.3739\n",
      "e: 15/100, lr: 0.001000, loss: 0.0234, col: 13.5362, vel: 201.8769, loss_val: 0.0318, col_val: 9.1710, vel_val: 44.6498\n",
      "e: 16/100, lr: 0.001000, loss: 0.0237, col: 14.4444, vel: 179.0423, loss_val: 0.0309, col_val: 8.8510, vel_val: 40.1167\n",
      "e: 17/100, lr: 0.001000, loss: 0.0269, col: 13.7168, vel: 194.6433, loss_val: 0.0349, col_val: 9.0487, vel_val: 40.5622\n",
      "e: 18/100, lr: 0.001000, loss: 0.0241, col: 14.2394, vel: 185.5484, loss_val: 0.0332, col_val: 8.8142, vel_val: 45.0500\n",
      "e: 19/100, lr: 0.001000, loss: 0.0241, col: 13.3029, vel: 171.3245, loss_val: 0.0339, col_val: 9.5662, vel_val: 35.9184\n",
      "e: 20/100, lr: 0.001000, loss: 0.0227, col: 13.1136, vel: 207.0795, loss_val: 0.0302, col_val: 9.3979, vel_val: 46.8447\n",
      "e: 21/100, lr: 0.001000, loss: 0.0359, col: 15.6068, vel: 99.9105, loss_val: 0.0432, col_val: 9.9258, vel_val: 22.5534\n",
      "e: 22/100, lr: 0.001000, loss: 0.0384, col: 16.2359, vel: 25.0531, loss_val: 0.0451, col_val: 9.6060, vel_val: 6.7055\n",
      "e: 23/100, lr: 0.001000, loss: 0.0372, col: 13.0351, vel: 22.5560, loss_val: 0.0389, col_val: 11.0416, vel_val: 5.1840\n",
      "e: 24/100, lr: 0.000100, loss: 0.0302, col: 10.8144, vel: 30.1505, loss_val: 0.0337, col_val: 10.7284, vel_val: 7.1920\n",
      "e: 25/100, lr: 0.000100, loss: 0.0283, col: 10.5539, vel: 35.7135, loss_val: 0.0328, col_val: 10.5571, vel_val: 8.2709\n",
      "e: 26/100, lr: 0.000100, loss: 0.0273, col: 10.4178, vel: 38.5476, loss_val: 0.0323, col_val: 10.3788, vel_val: 8.8068\n",
      "e: 27/100, lr: 0.000100, loss: 0.0268, col: 10.1417, vel: 40.2668, loss_val: 0.0322, col_val: 10.2871, vel_val: 9.0572\n",
      "e: 28/100, lr: 0.000100, loss: 0.0265, col: 9.8469, vel: 41.5721, loss_val: 0.0319, col_val: 10.0321, vel_val: 9.2689\n",
      "e: 29/100, lr: 0.000100, loss: 0.0259, col: 9.7821, vel: 41.6553, loss_val: 0.0317, col_val: 9.9358, vel_val: 9.1633\n",
      "e: 30/100, lr: 0.000100, loss: 0.0257, col: 9.6724, vel: 42.8508, loss_val: 0.0315, col_val: 9.7284, vel_val: 9.3294\n",
      "e: 31/100, lr: 0.000100, loss: 0.0275, col: 9.3571, vel: 5.7880, loss_val: 0.0330, col_val: 9.7582, vel_val: 1.1271\n",
      "e: 32/100, lr: 0.000100, loss: 0.0285, col: 8.7429, vel: 6.7911, loss_val: 0.0330, col_val: 9.4283, vel_val: 1.2995\n",
      "e: 33/100, lr: 0.000100, loss: 0.0282, col: 8.6033, vel: 7.8652, loss_val: 0.0329, col_val: 9.3099, vel_val: 1.5544\n",
      "e: 34/100, lr: 0.000100, loss: 0.0280, col: 8.5709, vel: 8.0009, loss_val: 0.0328, col_val: 9.2855, vel_val: 1.7714\n",
      "e: 35/100, lr: 0.000100, loss: 0.0281, col: 8.5037, vel: 7.4986, loss_val: 0.0329, col_val: 9.0455, vel_val: 1.5756\n",
      "e: 36/100, lr: 0.000100, loss: 0.0282, col: 8.3109, vel: 8.6809, loss_val: 0.0327, col_val: 9.0063, vel_val: 1.8874\n",
      "e: 37/100, lr: 0.000100, loss: 0.0283, col: 8.2541, vel: 8.2815, loss_val: 0.0332, col_val: 8.9711, vel_val: 1.7690\n",
      "e: 38/100, lr: 0.000100, loss: 0.0288, col: 8.0955, vel: 7.8377, loss_val: 0.0334, col_val: 8.8916, vel_val: 1.6434\n",
      "e: 39/100, lr: 0.000100, loss: 0.0288, col: 7.8927, vel: 8.0748, loss_val: 0.0334, col_val: 9.0102, vel_val: 1.7409\n",
      "e: 40/100, lr: 0.000100, loss: 0.0292, col: 7.6929, vel: 8.6828, loss_val: 0.0340, col_val: 9.0886, vel_val: 1.9367\n",
      "e: 41/100, lr: 0.000100, loss: 0.0316, col: 7.1120, vel: 3.7744, loss_val: 0.0361, col_val: 9.0919, vel_val: 0.5573\n",
      "e: 42/100, lr: 0.000100, loss: 0.0324, col: 6.4269, vel: 5.7155, loss_val: 0.0369, col_val: 9.0493, vel_val: 0.9636\n",
      "e: 43/100, lr: 0.000100, loss: 0.0316, col: 6.4876, vel: 3.1196, loss_val: 0.0361, col_val: 9.2259, vel_val: 0.5778\n",
      "e: 44/100, lr: 0.000100, loss: 0.0311, col: 6.4524, vel: 2.9457, loss_val: 0.0354, col_val: 9.1808, vel_val: 0.6517\n",
      "e: 45/100, lr: 0.000100, loss: 0.0307, col: 6.5317, vel: 2.8994, loss_val: 0.0350, col_val: 9.1553, vel_val: 0.8052\n",
      "e: 46/100, lr: 0.000100, loss: 0.0305, col: 6.4418, vel: 3.0916, loss_val: 0.0350, col_val: 9.1004, vel_val: 0.9183\n",
      "e: 47/100, lr: 0.000100, loss: 0.0304, col: 6.5317, vel: 2.8565, loss_val: 0.0351, col_val: 9.1294, vel_val: 1.1008\n",
      "e: 48/100, lr: 0.000100, loss: 0.0301, col: 6.3761, vel: 3.5274, loss_val: 0.0339, col_val: 8.9463, vel_val: 1.1016\n",
      "e: 49/100, lr: 0.000100, loss: 0.0299, col: 6.4097, vel: 3.4068, loss_val: 0.0339, col_val: 8.9994, vel_val: 0.8987\n",
      "e: 50/100, lr: 0.000100, loss: 0.0295, col: 6.4811, vel: 2.4820, loss_val: 0.0341, col_val: 9.1279, vel_val: 0.8550\n",
      "e: 51/100, lr: 0.000100, loss: 0.0315, col: 6.3209, vel: 2.7125, loss_val: 0.0346, col_val: 8.9399, vel_val: 0.8242\n",
      "e: 52/100, lr: 0.000100, loss: 0.0315, col: 6.1823, vel: 2.4335, loss_val: 0.0345, col_val: 8.9194, vel_val: 0.7082\n",
      "e: 53/100, lr: 0.000100, loss: 0.0312, col: 6.3047, vel: 2.2510, loss_val: 0.0349, col_val: 9.2106, vel_val: 0.8586\n",
      "e: 54/100, lr: 0.000100, loss: 0.0316, col: 6.1209, vel: 2.8119, loss_val: 0.0343, col_val: 8.9921, vel_val: 1.0688\n",
      "e: 55/100, lr: 0.000100, loss: 0.0314, col: 6.2745, vel: 2.1224, loss_val: 0.0340, col_val: 8.9766, vel_val: 1.0548\n",
      "e: 56/100, lr: 0.000100, loss: 0.0309, col: 6.1521, vel: 2.4616, loss_val: 0.0343, col_val: 9.1356, vel_val: 0.9933\n",
      "e: 57/100, lr: 0.000100, loss: 0.0315, col: 6.2227, vel: 1.9514, loss_val: 0.0345, col_val: 9.0488, vel_val: 1.0462\n",
      "e: 58/100, lr: 0.000100, loss: 0.0313, col: 6.1091, vel: 2.5908, loss_val: 0.0349, col_val: 9.0961, vel_val: 1.1348\n",
      "e: 59/100, lr: 0.000100, loss: 0.0312, col: 6.1770, vel: 2.1125, loss_val: 0.0346, col_val: 9.1016, vel_val: 1.1636\n",
      "e: 60/100, lr: 0.000100, loss: 0.0315, col: 6.0450, vel: 2.7126, loss_val: 0.0346, col_val: 8.9863, vel_val: 1.1025\n",
      "e: 61/100, lr: 0.000100, loss: 0.0332, col: 6.4530, vel: 2.0999, loss_val: 0.0352, col_val: 8.9308, vel_val: 0.9160\n",
      "e: 62/100, lr: 0.000100, loss: 0.0342, col: 5.8159, vel: 3.3569, loss_val: 0.0367, col_val: 8.9374, vel_val: 1.3932\n",
      "e: 63/100, lr: 0.000100, loss: 0.0337, col: 6.1335, vel: 1.5917, loss_val: 0.0363, col_val: 9.0107, vel_val: 1.1190\n",
      "e: 64/100, lr: 0.000100, loss: 0.0340, col: 6.0992, vel: 2.5181, loss_val: 0.0360, col_val: 8.8325, vel_val: 1.2969\n",
      "e: 65/100, lr: 0.000100, loss: 0.0337, col: 6.0472, vel: 2.5546, loss_val: 0.0361, col_val: 8.9347, vel_val: 1.3389\n",
      "e: 66/100, lr: 0.000100, loss: 0.0332, col: 5.9158, vel: 2.3995, loss_val: 0.0355, col_val: 8.8830, vel_val: 1.2598\n",
      "e: 67/100, lr: 0.000100, loss: 0.0332, col: 5.9770, vel: 2.1062, loss_val: 0.0359, col_val: 8.8097, vel_val: 1.2573\n",
      "e: 68/100, lr: 0.000100, loss: 0.0329, col: 5.9184, vel: 2.3572, loss_val: 0.0351, col_val: 8.8865, vel_val: 1.2381\n",
      "e: 69/100, lr: 0.000100, loss: 0.0330, col: 6.0371, vel: 2.0145, loss_val: 0.0361, col_val: 8.9355, vel_val: 1.4591\n",
      "e: 70/100, lr: 0.000100, loss: 0.0332, col: 5.8625, vel: 2.8766, loss_val: 0.0351, col_val: 8.6127, vel_val: 1.6814\n",
      "e: 71/100, lr: 0.000100, loss: 0.0360, col: 6.1670, vel: 1.5999, loss_val: 0.0370, col_val: 8.6827, vel_val: 1.7346\n",
      "e: 72/100, lr: 0.000100, loss: 0.0367, col: 6.1112, vel: 2.7892, loss_val: 0.0374, col_val: 8.7541, vel_val: 1.6652\n",
      "e: 73/100, lr: 0.000100, loss: 0.0366, col: 6.0112, vel: 2.4085, loss_val: 0.0383, col_val: 8.6808, vel_val: 1.3897\n",
      "e: 74/100, lr: 0.000100, loss: 0.0361, col: 5.8585, vel: 2.3732, loss_val: 0.0373, col_val: 8.6348, vel_val: 1.4139\n",
      "e: 75/100, lr: 0.000100, loss: 0.0368, col: 5.8597, vel: 1.7064, loss_val: 0.0378, col_val: 8.5292, vel_val: 1.5242\n",
      "e: 76/100, lr: 0.000100, loss: 0.0367, col: 5.7560, vel: 2.4579, loss_val: 0.0379, col_val: 8.7146, vel_val: 1.5545\n",
      "e: 77/100, lr: 0.000100, loss: 0.0373, col: 5.9380, vel: 1.5015, loss_val: 0.0386, col_val: 8.6780, vel_val: 1.3474\n",
      "e: 78/100, lr: 0.000100, loss: 0.0377, col: 5.6721, vel: 2.6979, loss_val: 0.0383, col_val: 8.5347, vel_val: 1.7493\n",
      "e: 79/100, lr: 0.000100, loss: 0.0379, col: 5.9382, vel: 1.3842, loss_val: 0.0393, col_val: 8.6992, vel_val: 1.2169\n",
      "e: 80/100, lr: 0.000100, loss: 0.0378, col: 5.7488, vel: 2.5651, loss_val: 0.0387, col_val: 8.5941, vel_val: 1.7489\n",
      "e: 81/100, lr: 0.000100, loss: 0.0404, col: 5.8473, vel: 2.1892, loss_val: 0.0388, col_val: 8.3936, vel_val: 1.8582\n",
      "e: 82/100, lr: 0.000100, loss: 0.0424, col: 5.7444, vel: 2.2608, loss_val: 0.0409, col_val: 8.3768, vel_val: 1.5715\n",
      "e: 83/100, lr: 0.000100, loss: 0.0427, col: 6.3333, vel: 1.0682, loss_val: 0.0423, col_val: 8.6624, vel_val: 1.6588\n",
      "e: 84/100, lr: 0.000100, loss: 0.0423, col: 5.8433, vel: 2.5997, loss_val: 0.0419, col_val: 8.3707, vel_val: 1.8446\n",
      "e: 85/100, lr: 0.000100, loss: 0.0424, col: 5.6817, vel: 1.8786, loss_val: 0.0419, col_val: 8.4430, vel_val: 1.3241\n",
      "e: 86/100, lr: 0.000100, loss: 0.0415, col: 5.6640, vel: 1.7886, loss_val: 0.0413, col_val: 8.4555, vel_val: 1.2336\n",
      "e: 87/100, lr: 0.000100, loss: 0.0414, col: 5.7245, vel: 1.6526, loss_val: 0.0414, col_val: 8.4525, vel_val: 1.2833\n",
      "e: 88/100, lr: 0.000100, loss: 0.0423, col: 5.6706, vel: 2.3102, loss_val: 0.0417, col_val: 8.3782, vel_val: 1.2497\n",
      "e: 89/100, lr: 0.000100, loss: 0.0427, col: 5.7710, vel: 1.4540, loss_val: 0.0426, col_val: 8.4258, vel_val: 1.2319\n",
      "e: 90/100, lr: 0.000100, loss: 0.0425, col: 5.5959, vel: 2.1692, loss_val: 0.0418, col_val: 8.4638, vel_val: 1.2342\n",
      "e: 91/100, lr: 0.000100, loss: 0.0438, col: 6.0796, vel: 1.2753, loss_val: 0.0451, col_val: 8.5869, vel_val: 1.2767\n",
      "e: 92/100, lr: 0.000100, loss: 0.0453, col: 5.6479, vel: 2.1892, loss_val: 0.0442, col_val: 8.4234, vel_val: 1.3072\n",
      "e: 93/100, lr: 0.000100, loss: 0.0455, col: 5.6662, vel: 2.4732, loss_val: 0.0453, col_val: 8.4557, vel_val: 1.6992\n",
      "e: 94/100, lr: 0.000100, loss: 0.0466, col: 5.6152, vel: 2.0855, loss_val: 0.0453, col_val: 8.2567, vel_val: 1.0006\n",
      "e: 95/100, lr: 0.000100, loss: 0.0464, col: 5.6211, vel: 1.5751, loss_val: 0.0458, col_val: 8.2945, vel_val: 0.8010\n",
      "e: 96/100, lr: 0.000100, loss: 0.0460, col: 5.6218, vel: 1.6520, loss_val: 0.0461, col_val: 8.3981, vel_val: 0.7706\n",
      "e: 97/100, lr: 0.000100, loss: 0.0463, col: 5.6075, vel: 1.9401, loss_val: 0.0466, col_val: 8.4038, vel_val: 0.8411\n",
      "e: 98/100, lr: 0.000100, loss: 0.0478, col: 5.6385, vel: 1.7212, loss_val: 0.0471, col_val: 8.1887, vel_val: 0.9056\n",
      "e: 99/100, lr: 0.000100, loss: 0.0479, col: 5.5864, vel: 2.2166, loss_val: 0.0479, col_val: 8.3432, vel_val: 1.0731\n",
      "e: 100/100, lr: 0.000100, loss: 0.0496, col: 5.5183, vel: 1.7734, loss_val: 0.0481, col_val: 8.2311, vel_val: 0.6723\n"
     ]
    }
   ],
   "source": [
    "metric_collector = MetricCollector()\n",
    "verbose = Verbose(metric_collector, optimizer, epochs)\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in batches:\n",
    "        batch_loss = train_step(batch[0], batch[1], params)\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "    # update stats\n",
    "    metric_collector.update_state(\n",
    "        tf.concat(state_train,0), \n",
    "        tf.concat(action_train,0), \n",
    "        tf.concat(state_test,0), \n",
    "        tf.concat(action_test,0), \n",
    "        params,\n",
    "    ) \n",
    "\n",
    "    # save best model\n",
    "    metric_collector.save_best_model(model_combined)\n",
    "\n",
    "    # update parameters\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        params[\"lambda_collision\"] = (\n",
    "            params[\"lambda_collision\"]\n",
    "            + params[\"eta_collision\"] * metric_collector.col[\"train\"][-1]\n",
    "        )\n",
    "        params[\"mu_collision\"] = params[\"mu_collision\"] * params[\"beta_collision\"]\n",
    "        params[\"lambda_velocity\"] = (\n",
    "            params[\"lambda_velocity\"]\n",
    "            + params[\"eta_velocity\"] * metric_collector.vel[\"train\"][-1]\n",
    "        )\n",
    "        params[\"mu_velocity\"] = params[\"mu_velocity\"] * params[\"beta_velocity\"]\n",
    "    \n",
    "    # print stats\n",
    "    verbose.on_batch_end(epoch+1, model_combined)\n",
    "    \n",
    "    # update learning rate\n",
    "    lr_scheduler.on_batch_end(metric_collector.col['test'][-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /home/local/ASUAD/kmajd1/SARP/examples/1_hospital_simulation/trained_models/repaired_policy/model/assets\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(current_dir + f\"/trained_models/repaired_policy\"):\n",
    "    os.makedirs(current_dir + f\"/trained_models/repaired_policy\")\n",
    "\n",
    "counter = 0\n",
    "for l in range(len(model_policy_orig.layers)):\n",
    "    if (len(model_policy_orig.layers[l].get_weights())) > 0:\n",
    "        model_policy_orig.layers[l].set_weights(\n",
    "            [metric_collector.best_weights[2*counter], metric_collector.best_weights[2*counter+1]]\n",
    "        )\n",
    "        counter += 1\n",
    "\n",
    "keras.models.save_model(\n",
    "    model_policy_orig,\n",
    "    f\"{current_dir}/trained_models/repaired_policy/model\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=False,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None,\n",
    "    save_traces=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
